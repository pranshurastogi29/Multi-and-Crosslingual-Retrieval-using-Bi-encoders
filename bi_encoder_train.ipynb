{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4cc4bc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:24:14.572060Z",
     "iopub.status.busy": "2024-12-24T21:24:14.571715Z",
     "iopub.status.idle": "2024-12-24T21:34:42.073033Z",
     "shell.execute_reply": "2024-12-24T21:34:42.071932Z"
    },
    "papermill": {
     "duration": 627.511501,
     "end_time": "2024-12-24T21:34:42.075460",
     "exception": false,
     "start_time": "2024-12-24T21:24:14.563959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyterrier\r\n",
      "  Downloading pyterrier-0.1.5-py2.py3-none-any.whl.metadata (9.3 kB)\r\n",
      "Requirement already satisfied: Click>=6.0 in /opt/conda/lib/python3.10/site-packages (from pyterrier) (8.1.7)\r\n",
      "Requirement already satisfied: jinja2>=2.10 in /opt/conda/lib/python3.10/site-packages (from pyterrier) (3.1.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.10->pyterrier) (2.1.5)\r\n",
      "Downloading pyterrier-0.1.5-py2.py3-none-any.whl (22 kB)\r\n",
      "Installing collected packages: pyterrier\r\n",
      "Successfully installed pyterrier-0.1.5\r\n",
      "Collecting unidecode\r\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\r\n",
      "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: unidecode\r\n",
      "Successfully installed unidecode-1.3.8\r\n",
      "Collecting sentence_transformers\r\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.46.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.26.2)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.5.15)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence_transformers) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.6.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\r\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: sentence_transformers\r\n",
      "Successfully installed sentence_transformers-3.3.1\r\n",
      "Collecting torch_scatter\r\n",
      "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: torch_scatter\r\n",
      "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for torch_scatter: filename=torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl size=3938487 sha256=8a131f6d278824689fbb888e0f014c24bbb97a9116c25f62bd3a6059b78e9e2e\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/92/f1/2b/3b46d54b134259f58c8363568569053248040859b1a145b3ce\r\n",
      "Successfully built torch_scatter\r\n",
      "Installing collected packages: torch_scatter\r\n",
      "Successfully installed torch_scatter-2.1.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyterrier\n",
    "!pip install unidecode\n",
    "!pip install sentence_transformers\n",
    "!pip install torch_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b7198f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:34:42.091932Z",
     "iopub.status.busy": "2024-12-24T21:34:42.091148Z",
     "iopub.status.idle": "2024-12-24T21:35:00.226488Z",
     "shell.execute_reply": "2024-12-24T21:35:00.225597Z"
    },
    "papermill": {
     "duration": 18.145649,
     "end_time": "2024-12-24T21:35:00.228621",
     "exception": false,
     "start_time": "2024-12-24T21:34:42.082972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict, field\n",
    "import os\n",
    "from typing import Any, Dict, Optional\n",
    "import yaml\n",
    "import re\n",
    "import string\n",
    "import datetime\n",
    "import ast\n",
    "from os.path import join as join_path\n",
    "from collections import *\n",
    "import argparse\n",
    "import copy\n",
    "import glob\n",
    "from functools import partial\n",
    "from statistics import mean, median, StatisticsError\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "from torch import nn\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, T5Config\n",
    "import collections\n",
    "from torch.optim import Optimizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import itertools\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "from torch_scatter import scatter\n",
    "import json\n",
    "from typing import *\n",
    "import math\n",
    "import random\n",
    "import statistics\n",
    "from scipy.stats import norm\n",
    "from argparse import Namespace\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import string\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "from unidecode import unidecode\n",
    "\n",
    "@dataclass\n",
    "class Model:\n",
    "    name: str = 'sentence-transformers/all-mpnet-base-v2'\n",
    "    pooling: str = 'default'  # 'mean', 'default'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Train:\n",
    "    device: str = 'cuda' \n",
    "    dtype: str = 'float32'  # float32, float16, bfloat16\n",
    "    batch_size: int = 8 \n",
    "    num_epochs: int = 20\n",
    "    metrics_window_size: int = 128\n",
    "    warmup_steps: int = 400\n",
    "    evaluation_steps: int = -1  # If -1, after each epoch\n",
    "    label_smoothing: float = 0.  # [0, 1]\n",
    "    ema: bool = False  # Exponential moving average\n",
    "    freezing: bool = False  # Gradually makes early modules untrainable\n",
    "    output_path: Optional[str] = os.path.join('/kaggle/working/')  # If None, no model checkpointing every evaluation_steps\n",
    "    save_vectors: bool = False  # Whether to save vectors for evaluation\n",
    "    save_models: bool = True # Whether to save models\n",
    "    p_max_seq_length: int = 512\n",
    "    fc_max_seq_length: int = 256\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "@dataclass\n",
    "class Optimizer:\n",
    "    name: str = 'adamw'  # adamw, shampoo_fb, shampoo_google\n",
    "    lr: float = 5e-6\n",
    "    weight_decay: float = 0.005 \n",
    "    clip_value: float = 1.0  # if None, no gradient clipping\n",
    "    sam: bool = False\n",
    "    sam_step: float = 1.  # Will be ignored if sam is False, propability of sam step\n",
    "    sam_rho: float = 0.05  # if sam_step=10 then rho=0.5,\n",
    "\n",
    "    \n",
    "@dataclass\n",
    "class Config:\n",
    "    model: Model = Model()\n",
    "    train: Train = Train()\n",
    "    optimizer: Optimizer = Optimizer()\n",
    "\n",
    "    seed: int = 3407\n",
    "\n",
    "    def __init__(self, path: Optional[str] = None):\n",
    "        self.timestamp = datetime.datetime.now().strftime('%d-%m-%Y-%H-%M-%S')\n",
    "        if path is not None: \n",
    "            with open(path, 'r') as f: self.init_class(yaml.load(f, Loader=yaml.SafeLoader))\n",
    "        \n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "    \n",
    "    def init_class(self, d):\n",
    "        for name in dir(self):\n",
    "            if name.startswith('_') or name.endswith('_') or name not in d:\n",
    "                continue\n",
    "            attr = getattr(self, name)\n",
    "            if isinstance(d[name], dict):\n",
    "                for k, v in d[name].items():\n",
    "                    setattr(attr, k, v)\n",
    "            else: \n",
    "                setattr(self, name, d[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9baaa5c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.244746Z",
     "iopub.status.busy": "2024-12-24T21:35:00.244175Z",
     "iopub.status.idle": "2024-12-24T21:35:00.260694Z",
     "shell.execute_reply": "2024-12-24T21:35:00.259677Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.026275,
     "end_time": "2024-12-24T21:35:00.262339",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.236064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Library of various cleaning-related functions, regular expressions and variables.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "simple_latin = string.ascii_lowercase + string.ascii_uppercase\n",
    "dirty_chars = string.digits + string.punctuation\n",
    "\n",
    "\n",
    "def is_clean_text(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Simple text cleaning method.\n",
    "    \"\"\"\n",
    "    dirty = (\n",
    "        len(text) < 25                                               # Short text\n",
    "        or\n",
    "        0.5 < sum(char in dirty_chars for char in text) / len(text)  # More than 50% dirty chars                                            \n",
    "    )\n",
    "    return not dirty\n",
    "\n",
    "\n",
    "# Source: https://gist.github.com/dperini/729294\n",
    "url_regex = re.compile(\n",
    "    r'(?:^|(?<![\\w\\/\\.]))'\n",
    "    r'(?:(?:https?:\\/\\/|ftp:\\/\\/|www\\d{0,3}\\.))'\n",
    "    r'(?:\\S+(?::\\S*)?@)?' r'(?:'\n",
    "    r'(?!(?:10|127)(?:\\.\\d{1,3}){3})'\n",
    "    r'(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})'\n",
    "    r'(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})'\n",
    "    r'(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])'\n",
    "    r'(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}'\n",
    "    r'(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))'\n",
    "    r'|'\n",
    "    r'(?:(?:[a-z\\\\u00a1-\\\\uffff0-9]-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)'\n",
    "    r'(?:\\.(?:[a-z\\\\u00a1-\\\\uffff0-9]-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)*'\n",
    "    r'(?:\\.(?:[a-z\\\\u00a1-\\\\uffff]{2,}))' r'|' r'(?:(localhost))' r')'\n",
    "    r'(?::\\d{2,5})?'\n",
    "    r'(?:\\/[^\\)\\]\\}\\s]*)?',\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "\n",
    "def remove_urls(text: str) -> str:\n",
    "    return url_regex.sub('', text)\n",
    "\n",
    "\n",
    "# Source: https://gist.github.com/Nikitha2309/15337f4f593c4a21fb0965804755c41d\n",
    "emoji_regex = re.compile('['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002500-\\U00002BEF'  # chinese char\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        u'\\U0001f926-\\U0001f937'\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u'\\u2640-\\u2642'\n",
    "        u'\\u2600-\\u2B55'\n",
    "        u'\\u200d'\n",
    "        u'\\u23cf'\n",
    "        u'\\u23e9'\n",
    "        u'\\u231a'\n",
    "        u'\\ufe0f'  # dingbats\n",
    "        u'\\u3030'\n",
    "    ']+')\n",
    "\n",
    "\n",
    "def remove_emojis(text: str) -> str:\n",
    "    return emoji_regex.sub('', text)\n",
    "\n",
    "\n",
    "sentence_stop_regex = re.compile('['\n",
    "    u'\\u002e' # full stop\n",
    "    u'\\u2026' # ellipsis\n",
    "    u'\\u061F' # arabic question mark\n",
    "    u'\\u06D4' # arabic full stop\n",
    "    u'\\u2022' # bullet point\n",
    "    u'\\u3002' # chinese period\n",
    "    u'\\u25CB' # white circle\n",
    "    '\\|'      # pipe\n",
    "']+')\n",
    "\n",
    "\n",
    "def replace_stops(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces some characters that are being used to end sentences. Used for sentence segmentation with sliding windows.\n",
    "    \"\"\"\n",
    "    return sentence_stop_regex.sub('.', text)\n",
    "\n",
    "\n",
    "whitespace_regex = re.compile(r'\\s+')\n",
    "\n",
    "\n",
    "def replace_whitespaces(text: str) -> str:\n",
    "    return whitespace_regex.sub(' ', text)\n",
    "\n",
    "\n",
    "def clean_ocr(ocr: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove all lines that are shorter than 6 and have more than 50% `dirty_chars`.\n",
    "    \"\"\"\n",
    "    return '\\n'.join(\n",
    "        line\n",
    "        for line in ocr.split('\\n')\n",
    "        if len(line) > 5 and sum(char in dirty_chars for char in line) / len(line) < 0.5\n",
    "    )\n",
    "\n",
    "\n",
    "def clean_twitter_picture_links(text):\n",
    "    \"\"\"\n",
    "    Replaces links to picture in twitter post only with 'pic'. \n",
    "    \"\"\"\n",
    "    return re.sub(r'pic.twitter.com/\\S+', 'pic', text)\n",
    "\n",
    "\n",
    "def clean_twitter_links(text):\n",
    "    \"\"\"\n",
    "    Replaces twitter links with 't.co'.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\S+//t.co/\\S+', 't.co', text)\n",
    "\n",
    "\n",
    "def remove_elongation(text):\n",
    "    \"\"\"\n",
    "    Replaces any occurrence of a string of consecutive identical non-space \n",
    "    characters (at least three in a row) with just one instance of that character.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'(\\S+)\\1{2,}', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    try:\n",
    "        return ast.literal_eval(str(value))\n",
    "    except (ValueError, SyntaxError):\n",
    "        return value  # Or `None`, depending on how you want to handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "347abfb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.278893Z",
     "iopub.status.busy": "2024-12-24T21:35:00.278639Z",
     "iopub.status.idle": "2024-12-24T21:35:00.285280Z",
     "shell.execute_reply": "2024-12-24T21:35:00.284570Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.016129,
     "end_time": "2024-12-24T21:35:00.286869",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.270740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Library of custom types used for datasets. Some basic functions over these types are also included.\n",
    "\"\"\"\n",
    "\n",
    "Id2FactCheck = Dict[int, str]\n",
    "Id2Post = Dict[int, str]\n",
    "FactCheckPostMapping = List[Tuple[int, int]]\n",
    "\n",
    "Language = str  # ISO 639-3 language code\n",
    "LanguageDistribution = Dict[Language, float]  # Confidence for language identification. Should sum to [0, 1].\n",
    "\n",
    "OriginalText = str\n",
    "EnglishTranslation = str\n",
    "TranslatedText = Tuple[OriginalText, EnglishTranslation, LanguageDistribution]\n",
    "\n",
    "Instance = Tuple[Optional[datetime.datetime], str]  # When and where was a fact-check or post published\n",
    "\n",
    "\n",
    "def is_in_distribution(language: Language, distribution: LanguageDistribution, threshold: float = 0.2) -> bool:\n",
    "    \"\"\"\n",
    "    Check whether `language` is in a `distribution` with more than `treshold` x 100%\n",
    "    \"\"\"\n",
    "    return next(\n",
    "        (\n",
    "            percentage >= threshold\n",
    "            for distribution_language, percentage in distribution\n",
    "            if distribution_language == language\n",
    "        ),\n",
    "        False\n",
    "    )\n",
    "\n",
    "\n",
    "def combine_distributions(texts: Iterable[TranslatedText]) -> LanguageDistribution:\n",
    "    \"\"\"\n",
    "    Combine `LanguageDistribution`s from multiple `TranslatedText`s taking the length of the text into consideration.\n",
    "    \"\"\"\n",
    "    total_length = sum(len(text[0]) for text in texts)\n",
    "    distribution = defaultdict(lambda: 0)\n",
    "    for original_text, _, text_distribution in texts:\n",
    "        for language, percentage in text_distribution:\n",
    "            distribution[language] += percentage * len(original_text) / total_length\n",
    "    return list(distribution.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a005e0fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.301967Z",
     "iopub.status.busy": "2024-12-24T21:35:00.301717Z",
     "iopub.status.idle": "2024-12-24T21:35:00.309750Z",
     "shell.execute_reply": "2024-12-24T21:35:00.309095Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.017244,
     "end_time": "2024-12-24T21:35:00.311173",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.293929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"Dataset\n",
    "    \n",
    "    Abstract class for datasets. Subclasses should implement `load` function that load `id_to_fact_check`, `id_to_post`,\n",
    "    and `fact_check_post_mapping` object attributes. The class also implemenets basic cleaning methods that might be\n",
    "    reused.\n",
    "    \n",
    "    Attributes:\n",
    "        clean_ocr: bool = True  Should cleaning of OCRs be performed\n",
    "        remove_emojis: bool = False  Should emojis be removed from the texts?\n",
    "        remove_urls: bool = True  Should URLs be removed from the texts?\n",
    "        replace_whitespaces: bool = True  Should whitespaces be replaced by a single space whitespace?\n",
    "        clean_twitter: bool = True  \n",
    "        remove_elongation: bool = False  Should occurrence of a string of consecutive identical non-space \n",
    "    characters (at least three in a row) with just one instance of that character?\n",
    "\n",
    "        After `load` is called, following attributes are accesible:\n",
    "                fact_check_post_mapping: list[tuple[int, int]]  List of Factcheck-Post id pairs.\n",
    "                id_to_fact_check: dict[int, str]  Factcheck id -> Factcheck text\n",
    "                id_to_post: dict[int, str]  Post id -> Post text\n",
    "                \n",
    "    Methods:\n",
    "        clean_text: Performs text cleaning based on initialization attributes.\n",
    "        maybe_clean_ocr: Perform OCR-specific text cleaning, if `self.clean_ocr`\n",
    "        load: Abstract method. To be implemented by the subclasses.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # The default values here are based on our preliminary experiments. Might not be the best for all cases.\n",
    "    def __init__(\n",
    "        self,\n",
    "        clean_ocr: bool = True,\n",
    "        dataset: str = None,  # Here to read and discard the `dataset` field from the argparser\n",
    "        remove_emojis: bool = True,\n",
    "        remove_urls: bool = True,\n",
    "        replace_whitespaces: bool = True,\n",
    "        clean_twitter: bool = True,\n",
    "        remove_elongation: bool = False\n",
    "    ):\n",
    "        self.clean_ocr = clean_ocr\n",
    "        self.remove_emojis = remove_emojis\n",
    "        self.remove_urls = remove_urls\n",
    "        self.replace_whitespaces = replace_whitespaces\n",
    "        self.clean_twitter = clean_twitter\n",
    "        self.remove_elongation = remove_elongation\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fact_check_post_mapping)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        p_id, fc_id = self.fact_check_post_mapping[idx]\n",
    "        return self.id_to_fact_check[fc_id], self.id_to_post[p_id]\n",
    "\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \n",
    "        if self.remove_urls:\n",
    "            text = remove_urls(text)\n",
    "\n",
    "        if self.remove_emojis:\n",
    "            text = remove_emojis(text)\n",
    "\n",
    "        if self.replace_whitespaces:\n",
    "            text = replace_whitespaces(text)\n",
    "        \n",
    "        if self.clean_twitter:\n",
    "            text = clean_twitter_picture_links(text)\n",
    "            text = clean_twitter_links(text)\n",
    "        \n",
    "        if self.remove_elongation:\n",
    "            text = remove_elongation(text)\n",
    "\n",
    "        return text.strip()        \n",
    "        \n",
    "        \n",
    "    def maybe_clean_ocr(self, ocr):\n",
    "        if self.clean_ocr:\n",
    "            return clean_ocr(ocr)\n",
    "        return ocr\n",
    "        \n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        if name in {'id_to_fact_check', 'id_to_post', 'fact_check_post_mapping'}:\n",
    "            raise AttributeError(f\"You have to `load` the dataset first before using '{name}'\")\n",
    "        raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}'\")\n",
    "\n",
    "        \n",
    "    def load(self):\n",
    "        raise NotImplementedError\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55f1b0d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.326293Z",
     "iopub.status.busy": "2024-12-24T21:35:00.326080Z",
     "iopub.status.idle": "2024-12-24T21:35:00.343623Z",
     "shell.execute_reply": "2024-12-24T21:35:00.342991Z"
    },
    "papermill": {
     "duration": 0.027094,
     "end_time": "2024-12-24T21:35:00.345141",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.318047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class OurDataset(Dataset):\n",
    "    \"\"\"Our dataset\n",
    "    \n",
    "    Class for our dataset that can load different variants. It requires data from `drive/datasets/ours`.\n",
    "\n",
    "    Initialization Attributes:\n",
    "        crosslingual: bool  If `True`, only crosslingual pairs (fact-check and post in different languages) are loaded.\n",
    "        fact_check_fields: Iterable[str]  List of fields used to generate the final `str` representation for fact-checks. Supports `claim` and `title`.\n",
    "        fact_check_language: Optional[Language]  If a `Language` is specified, only fact-checks with that language are selected.\n",
    "        language: Optional[Language]  If a `Language` is specified, only fact-checks and posts with that language are selected.\n",
    "        post_language: Optional[Language]  If a `Language` is specified, only posts with that language are selected.\n",
    "        split: `train`, `test` or `dev`. `None` means all the samples.\n",
    "        version: 'original' or 'english'. Language version of the dataset.\n",
    "        \n",
    "        Also check `Dataset` attributes.\n",
    "        \n",
    "        After `load` is called, following attributes are accesible:\n",
    "            fact_check_post_mapping: list[tuple[int, int]]  List of Factcheck-Post id pairs.\n",
    "            id_to_fact_check: dict[int, str]  Factcheck id -> Factcheck text\n",
    "            id_to_post: dict[int, str]  Post id -> Post text\n",
    "        \n",
    "\n",
    "    Methods:\n",
    "        load: Loads the data from the csv files. Populates `id_to_fact_check`, `id_to_post` and `fact_check_post_mapping` attributes.\n",
    "    \"\"\"\n",
    "        \n",
    "    our_dataset_path = join_path('/semeval-data/SemEval_Task7/')\n",
    "    csvs_loaded = False\n",
    "\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        crosslingual: bool = False,\n",
    "        fact_check_fields: Iterable[str] = ('claim', 'title'),\n",
    "        fact_check_language: Optional[Language] = None,\n",
    "        language: Optional[Language] = None,\n",
    "        post_language: Optional[Language] = None,\n",
    "        split: Optional[str] = None,\n",
    "        # version: str = 'original',\n",
    "        fold: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        assert all(field in ('claim', 'title') for field in fact_check_fields)\n",
    "        assert split in ('test', 'train')\n",
    "        \n",
    "        # self.crosslingual = crosslingual\n",
    "        self.split = split\n",
    "        self.fold = fold\n",
    "        \n",
    "    @classmethod\n",
    "    def maybe_load_csvs(cls):\n",
    "        \"\"\"\n",
    "        Load the csvs and store them as class variables. When individual objects are initialized, they can reuse the same\n",
    "        pre-loaded dataframes without costly text parsing.\n",
    "        \n",
    "        `OurDataset.csvs_loaded` is a flag indicating whether the csvs are already loaded.\n",
    "        \"\"\"\n",
    "        \n",
    "        if cls.csvs_loaded:\n",
    "            return\n",
    "        \n",
    "        posts_path = join_path(cls.our_dataset_path, 'posts.csv')\n",
    "        fact_checks_path = join_path(cls.our_dataset_path, 'fact_checks.csv')\n",
    "        fact_check_post_mapping_path = join_path(cls.our_dataset_path, 'pairs.csv')\n",
    "        \n",
    "        for path in [posts_path, fact_checks_path, fact_check_post_mapping_path]:\n",
    "            assert os.path.isfile(path)\n",
    "\n",
    "        \n",
    "        parse_col = lambda s: ast.literal_eval(s.replace('\\n', '\\\\n')) if s else s\n",
    "        \n",
    "        print('Loading fact-checks.')\n",
    "        cls.df_fact_checks = pd.read_csv(fact_checks_path).fillna('').set_index('fact_check_id')\n",
    "        for col in ['claim', 'instances', 'title']:\n",
    "            cls.df_fact_checks[col] = cls.df_fact_checks[col].apply(parse_col)\n",
    "        print(f'{len(cls.df_fact_checks)} loaded.')\n",
    "\n",
    "            \n",
    "        print('Loading posts.')\n",
    "        cls.df_posts = pd.read_csv(posts_path).fillna('').set_index('post_id')\n",
    "        for col in ['instances', 'ocr', 'verdicts', 'text']:\n",
    "            cls.df_posts[col] = cls.df_posts[col].apply(parse_col)\n",
    "        print(f'{len(cls.df_posts)} loaded.')\n",
    "\n",
    "         \n",
    "        print('Loading fact-check-post mapping.')\n",
    "        cls.df_fact_check_post_mapping = pd.read_csv(fact_check_post_mapping_path) \n",
    "        print(f'{len(cls.df_fact_check_post_mapping)} loaded.')\n",
    "        \n",
    "        cls.csvs_loaded = True\n",
    "        \n",
    "\n",
    "    def load(self):\n",
    "        \n",
    "        self.maybe_load_csvs()\n",
    "        \n",
    "        df_posts = self.df_posts.copy()\n",
    "        df_fact_checks = self.df_fact_checks.copy()\n",
    "        df_fact_check_post_mapping = self.df_fact_check_post_mapping.copy()\n",
    "\n",
    "        df_posts['text'] = df_posts['text'].apply(lambda x: safe_literal_eval(x))\n",
    "        df_posts['ocr'] = df_posts['ocr'].apply(lambda x: safe_literal_eval(x))\n",
    "        df_fact_checks['claim'] = df_fact_checks['claim'].apply(lambda x: safe_literal_eval(x))\n",
    "        df_fact_checks['title'] = df_fact_checks['title'].apply(lambda x: safe_literal_eval(x))\n",
    "\n",
    "        train_fold = pd.read_csv(f'/semeval-data/train_fold_task_7/fold_{self.fold}_train.csv')\n",
    "        test_fold = pd.read_csv(f'/semeval-data/train_fold_task_7/fold_{self.fold}_test.csv')\n",
    "        # full_data = pd.concat([train_fold, test_fold]).reset_index(drop = True)\n",
    "      \n",
    "        if self.split:\n",
    "            fold_data = pd.read_csv(f'/semeval-data/train_fold_task_7/fold_{self.fold}_{self.split}.csv')\n",
    "            df_posts = df_posts[df_posts.index.isin(fold_data['post_id'].values)]\n",
    "            \n",
    "            df_fact_check_post_mapping = df_fact_check_post_mapping[df_fact_check_post_mapping['post_id'].isin(fold_data['post_id'].values)]\n",
    "            \n",
    "            df_fact_checks_1 = df_fact_checks[df_fact_checks.index.isin(fold_data['fact_check_id'].unique())]\n",
    "            df_fact_checks_2 = df_fact_checks[~(df_fact_checks.index.isin(fold_data['fact_check_id'].unique()))]\n",
    "            df_fact_checks = pd.concat([df_fact_checks_1, df_fact_checks_2.sample(n = .30, random_state = 3407)])\n",
    "    \n",
    "            print(f'Filtering by split: {len(df_posts)} posts remaining and sampled {len(df_fact_checks)} fact checks')\n",
    "\n",
    "        print(f'Filtering fact-checks by language: {len(df_fact_checks)} posts remaining.')\n",
    "        print(f'Filtering posts by language: {len(df_posts)} posts remaining.')\n",
    "        \n",
    "\n",
    "        # Create mapping variable\n",
    "        post_ids = set(df_posts.index)\n",
    "        fact_check_ids = set(df_fact_checks.index)\n",
    "        fact_check_post_mapping = set((fact_check_id, post_id) \n",
    "                                      for fact_check_id, post_id in df_fact_check_post_mapping.itertuples(index=False, name=None))\n",
    "        print(f'Mappings remaining: {len(fact_check_post_mapping)}.')\n",
    "\n",
    "        print(f'Filtering posts.')\n",
    "        print(f'Posts remaining: {len(df_posts)}')\n",
    "            \n",
    "    \n",
    "        # Create object attributes\n",
    "        self.fact_check_post_mapping = list(fact_check_post_mapping)\n",
    "\n",
    "        self.id_to_post = dict()\n",
    "        for post_id, post_text, ocr_text in zip(df_posts.index, df_posts['text'], df_posts['ocr']):\n",
    "            texts = list()\n",
    "            if post_text:\n",
    "                texts.append(self.maybe_clean_ocr(post_text[1]))\n",
    "            if ocr_text:\n",
    "                texts.append(self.maybe_clean_ocr(ocr_text[0][1]))\n",
    "            self.id_to_post[post_id] = self.clean_text(' '.join(texts))\n",
    "\n",
    "        self.id_to_fact_check = dict()\n",
    "        for fact_check_id, claim, title in zip(df_fact_checks.index, df_fact_checks['claim'], df_fact_checks['title']):\n",
    "            texts = list()\n",
    "            if claim:\n",
    "                texts.append(self.maybe_clean_ocr(claim[1]))\n",
    "            if title:\n",
    "                texts.append(self.maybe_clean_ocr(title[1]))\n",
    "            self.id_to_fact_check[fact_check_id] = self.clean_text(' '.join(texts))\n",
    "            \n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9289d47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.359933Z",
     "iopub.status.busy": "2024-12-24T21:35:00.359695Z",
     "iopub.status.idle": "2024-12-24T21:35:00.364636Z",
     "shell.execute_reply": "2024-12-24T21:35:00.363981Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.014142,
     "end_time": "2024-12-24T21:35:00.366067",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.351925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DummyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Very small dataset based on OurDataset with 100 fact-checks an 100 posts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def load(self):\n",
    "        \n",
    "        dt = OurDataset(split='test').load()\n",
    "        dt.fact_check_post_mapping = list(dt.fact_check_post_mapping)[:100]\n",
    "        \n",
    "        fact_check_ids, post_ids = map(set, zip(*dt.fact_check_post_mapping))\n",
    "        \n",
    "        dt.id_to_fact_check = {\n",
    "            k: v\n",
    "            for k, v in dt.id_to_fact_check.items()\n",
    "            if k in fact_check_ids\n",
    "        }\n",
    "        \n",
    "        dt.id_to_post = {\n",
    "            k: v\n",
    "            for k, v in dt.id_to_post.items()\n",
    "            if k in post_ids\n",
    "        }\n",
    "        return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3343dde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.386964Z",
     "iopub.status.busy": "2024-12-24T21:35:00.386743Z",
     "iopub.status.idle": "2024-12-24T21:35:00.398105Z",
     "shell.execute_reply": "2024-12-24T21:35:00.397192Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.025101,
     "end_time": "2024-12-24T21:35:00.400454",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.375353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A library of metric-calculating functions. The default result format is Iterable[List[int]] -- an iterable of results for individual queries (posts).\n",
    "Each query has a list of ranks assigned, representing the ranks of appropriate documents (fact-checks).\n",
    "\"\"\"\n",
    "\n",
    "def binary_ci(success: int, total: int, alpha: float = 0.95) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Using Agresti-Coull interval\n",
    "    \n",
    "    Return mean and confidence interval (lower and upper bound)\n",
    "    \"\"\"\n",
    "    z = statistics.NormalDist().inv_cdf((1 + alpha) / 2)\n",
    "    total = total + z**2\n",
    "    loc = (success + (z**2) / 2) / total\n",
    "    diameter = z * math.sqrt(loc * (1 - loc) / total)\n",
    "    return loc, loc - diameter, loc + diameter \n",
    "\n",
    "\n",
    "def bootstrap_ci(scores, alpha=0.95) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Bootstrapping based estimate.\n",
    "    \n",
    "    Return mean and confidence interval (lower and upper bound)\n",
    "    \"\"\"\n",
    "    loc, scale = norm.fit(scores)    \n",
    "    bootstrap = [sum(random.choices(scores, k=len(scores))) / len(scores) for _ in range(1000)]\n",
    "    lower, upper = norm.interval(alpha, *norm.fit(bootstrap))\n",
    "        \n",
    "    return loc, lower, upper\n",
    "\n",
    "\n",
    "def pair_success_at_k(ranks, k=10):\n",
    "    \"\"\"\n",
    "    Pair S@K - How many fact-check-post pairs from all the pairs ended up in the top K.\n",
    "    \"\"\"\n",
    "    values = [rank <= k for query in ranks for rank in query]\n",
    "    return binary_ci(sum(values), len(values))\n",
    "\n",
    "        \n",
    "def post_success_at_k(ranks, k=10):\n",
    "    \"\"\"\n",
    "    Post S@K - For how many posts at least one pair ended up in the top K.\n",
    "    \"\"\"\n",
    "    values = [any(rank <= k for rank in query) for query in ranks]\n",
    "    return binary_ci(sum(values), len(values))\n",
    "\n",
    "        \n",
    "def precision_at_k(ranks, k=10):\n",
    "    \"\"\"\n",
    "    P@K - How many positive hits in the top K\n",
    "    \"\"\"\n",
    "    values = [sum(rank <= k for rank in query) for query in ranks]\n",
    "    return binary_ci(sum(values), len(values) * k)\n",
    "\n",
    "        \n",
    "def mrr(ranks):\n",
    "    \"\"\"\n",
    "    Mean Reciprocal Rank: 1/r for r in ranks\n",
    "    \"\"\"\n",
    "    values = [1 / min(query) for query in ranks]\n",
    "    return bootstrap_ci(values)\n",
    "\n",
    "        \n",
    "def map_(ranks):\n",
    "    \"\"\"\n",
    "    Mean Average Precision: As defined here page 7: https://datascience-intro.github.io/1MS041-2022/Files/AveragePrecision.pdf\n",
    "    \"\"\"\n",
    "    values = [\n",
    "        np.mean([\n",
    "            (i + 1) / rank\n",
    "            for i, rank in enumerate(sorted(query))\n",
    "        ])\n",
    "        for query in ranks\n",
    "    ]\n",
    "    return bootstrap_ci(values)\n",
    "\n",
    "\n",
    "def map_k(ranks, k=5):\n",
    "    values = []\n",
    "    for query in ranks:\n",
    "        ap_at_k = 0\n",
    "        num_correct = 0\n",
    "        for i, rank in enumerate(query):\n",
    "            if rank <= k:\n",
    "                num_correct += 1\n",
    "                ap_at_k += num_correct / (i+1)\n",
    "        values.append(ap_at_k)\n",
    "    return bootstrap_ci(values)\n",
    "\n",
    "\n",
    "def standard_metrics(ranks: Iterable[List[int]]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate several metrics and their CIs based on the ranks provided\n",
    "    \n",
    "    Attributes:\n",
    "        ranks - Iterable of results for individual queries. For each query a list of ranks is expected.\n",
    "    \"\"\"\n",
    "        \n",
    "    return {\n",
    "        'pair_success_at_10': pair_success_at_k(ranks),\n",
    "        'post_success_at_10': post_success_at_k(ranks),\n",
    "        'precision_at_10': precision_at_k(ranks),\n",
    "    }\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b9ec6eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.423436Z",
     "iopub.status.busy": "2024-12-24T21:35:00.423022Z",
     "iopub.status.idle": "2024-12-24T21:35:00.432053Z",
     "shell.execute_reply": "2024-12-24T21:35:00.431242Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.022011,
     "end_time": "2024-12-24T21:35:00.434820",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.412809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A library of functions that can be used to evaluate the results produced by _result generators_. Result generators are all the different methods\n",
    "that can be used to retrieve fact-checks.\n",
    "\n",
    "`process_result_generator` is the main API that should be used to create evaluation results.\n",
    "\n",
    "Currently supported:\n",
    "    bm25 - BM25\n",
    "    dummy - Returns fixed order of fact-checks. Used for debugging.\n",
    "    embedding - General method that supports different embedding models and then calculate cosine similarity between the vectors.\n",
    "\"\"\"\n",
    "\n",
    "def predicted_ranks(predicted_ids: np.array, desired_ids: np.array, default_rank: int = None):\n",
    "    \"\"\"\n",
    "    Return sorted ranks of the `desired_ids` in the `predicted_ids` array.\n",
    "    \n",
    "    If `default_rank` is set, the final array will be padded with the value for all the ids that were not present in the `predicted_ids` array.\n",
    "    \"\"\"\n",
    "    \n",
    "    predicted_ranks = dict()\n",
    "    \n",
    "    for desired in desired_ids:\n",
    "        \n",
    "        try:\n",
    "            rank = np.where(predicted_ids == desired)[0][0] + 1  # +1 so that the first item has rank 1, not 0\n",
    "        except IndexError:\n",
    "            rank = default_rank\n",
    "       \n",
    "        if rank is not None:\n",
    "            predicted_ranks[desired] = rank\n",
    "        \n",
    "    return predicted_ranks\n",
    "\n",
    "\n",
    "def process_result_generator(gen: Generator, default_rank: int = None, csv_path: str = None):\n",
    "    \"\"\"\n",
    "    Take the results generated from `gen` and process them. By default, only calculate metrics, but dumping the results into a csv file is also supported\n",
    "    via `csv_path` attribute. For `default_rank` see `predicted_ranks` function.\n",
    "    \"\"\"\n",
    "    \n",
    "    ranks = list()\n",
    "    rows = list()\n",
    "    \n",
    "    for predicted_ids, desired_ids, post_id in gen:\n",
    "        post_ranks = predicted_ranks(predicted_ids, desired_ids, default_rank)\n",
    "        ranks.append(post_ranks.values())\n",
    "        \n",
    "        if csv_path:\n",
    "            rows.append((post_id, post_ranks, predicted_ids[:50]))\n",
    "            \n",
    "    print(f'{sum(len(query) for query in ranks)} ranks produced.')\n",
    "            \n",
    "    if csv_path:\n",
    "        pd.DataFrame(rows, columns=['post_id', 'desired_fact_check_ranks', 'predicted_fact_check_ids']).to_csv(csv_path, index=False)\n",
    "      \n",
    "    return standard_metrics(ranks)\n",
    "\n",
    "\n",
    "def result_generator(func):\n",
    "    \"\"\"\n",
    "    This is a decorator function that should be used on result generators. The generators return by default: `predicted_fact_check_ids` and `post_id`.\n",
    "    Here, the results are enriched with the `desired_fact_check_ids` as indicated by `dataset.fact_check_post_mapping`\n",
    "    \"\"\"\n",
    "    \n",
    "    def wrapper(dataset, *args, **kwargs):\n",
    "        \n",
    "        desired_fact_check_ids = defaultdict(lambda: list())\n",
    "        for fact_check_id, post_id in dataset.fact_check_post_mapping:\n",
    "            desired_fact_check_ids[post_id].append(fact_check_id)\n",
    "        \n",
    "        for predicted_fact_check_ids, post_id in func(dataset, *args, **kwargs):\n",
    "            yield predicted_fact_check_ids, desired_fact_check_ids[post_id], post_id\n",
    "        \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "183f969a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.454796Z",
     "iopub.status.busy": "2024-12-24T21:35:00.454494Z",
     "iopub.status.idle": "2024-12-24T21:35:00.464637Z",
     "shell.execute_reply": "2024-12-24T21:35:00.463873Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.019876,
     "end_time": "2024-12-24T21:35:00.466143",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.446267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    \"\"\"\n",
    "    An abstract class for vectorizers. Vectorizers calculate vectors for texts and handle thir caching so that we do not have to calculate\n",
    "    the vector for the same text more than once.\n",
    "    \n",
    "    Currently supported:\n",
    "            `SentenceTransformerVectorizer` - Used for models using `sentence_transformers` library.\n",
    "            `LaserVectorizer` - TBA\n",
    "            `PytorchVectorizer` - Used for pytorch models (nn.Module).\n",
    "            \n",
    "    The main call for `Vectorizer` is `vectorize`. This will calculate the appropriate vectors and store them in the `dict` attribute. The class also\n",
    "    supports `save` and `load`. `dir_path` is used as path to a folder where there is a `vocab.json` stored with the collection of texts and `vectors.py`\n",
    "    with a torch tensor of vectors for the texts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dir_path: str):\n",
    "        self.dict = {}\n",
    "        \n",
    "        self.dir_path = dir_path\n",
    "        if dir_path:\n",
    "            self.vectors_path = os.path.join(dir_path, 'vectors.pt')\n",
    "            self.vocab_path = os.path.join(dir_path, 'vocab.json')\n",
    "        \n",
    "            try:\n",
    "                self.load()\n",
    "                print(f'Vector database with {len(self.dict)} records loaded')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "                print(f'No previous database found')\n",
    "\n",
    "        \n",
    "    def vectorize(self, texts: List[str], save_if_missing: bool = False, normalize: bool = False) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        The main API point for the users. Try to find the vectors in the existing database. For the missing texts, the vectors will be calculated\n",
    "        and saved in the `self.dict`. \n",
    "        \n",
    "        Attributes:\n",
    "            save_if_missing: bool  Should the vectors in `dict` be saved after new vectors are calculated? This makes sense for models that will\n",
    "                be used more than once.\n",
    "            normalize: bool  Should the vectors be normalized. Useful for cosine similarity calculations.\n",
    "        \"\"\"\n",
    "        \n",
    "        missing_texts = list(set(texts) - set(self.dict.keys()))\n",
    "        \n",
    "        if missing_texts:\n",
    "            \n",
    "            print(f'Calculating {len(missing_texts)} vectors.')\n",
    "            missing_vectors = self._calculate_vectors(missing_texts)\n",
    "            for text, vector in zip(missing_texts, missing_vectors):\n",
    "                self.dict[text] = vector\n",
    "            \n",
    "            if save_if_missing:\n",
    "                self.save()\n",
    "            \n",
    "        vectors = torch.vstack([\n",
    "            self.dict[text]\n",
    "            for text in texts\n",
    "        ])\n",
    "        \n",
    "        if normalize:\n",
    "            vectors = torch.nn.functional.normalize(vectors, p=2, dim=1)\n",
    "            \n",
    "        return vectors\n",
    "\n",
    "    \n",
    "    def _calculate_vectors(self, txts: List[str]) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Abstract method to be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "            \n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Load vocab and vectors from appropriate files\n",
    "        \"\"\"\n",
    "        with open(self.vocab_path, 'r', encoding='utf8') as f:\n",
    "            vocab = json.load(f)\n",
    "        \n",
    "        vectors = torch.load(self.vectors_path)\n",
    "        \n",
    "        assert len(vocab) == len(vectors)\n",
    "        \n",
    "        self.dict = {\n",
    "            text: vector\n",
    "            for text, vector in zip(vocab, vectors)\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        Save vocab and vectors to appropriate files\n",
    "        \"\"\"\n",
    "        os.makedirs(self.dir_path, exist_ok=True)\n",
    "            \n",
    "        vocab = list(self.dict.keys())        \n",
    "        with open(self.vocab_path, 'w', encoding='utf8') as f:\n",
    "            json.dump(vocab, f)\n",
    "            \n",
    "        vectors = torch.vstack(list(self.dict.values()))\n",
    "        torch.save(vectors, self.vectors_path)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee9b61e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.480911Z",
     "iopub.status.busy": "2024-12-24T21:35:00.480682Z",
     "iopub.status.idle": "2024-12-24T21:35:00.496251Z",
     "shell.execute_reply": "2024-12-24T21:35:00.495536Z"
    },
    "papermill": {
     "duration": 0.024775,
     "end_time": "2024-12-24T21:35:00.497693",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.472918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def slice_text(text, window_type, window_size, window_stride=None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split a `text` into parts using a sliding window. The windows slides either across characters or sentences, based on the value of `window_tyoe`.\n",
    "    \n",
    "    Attributes:\n",
    "        text: str  Text that is to be splitted into windows.\n",
    "        window_type: str  Either `sentence` or `character`. The basic unit of the windows.\n",
    "        window_size: int  How many units are in a window.\n",
    "        window_stride: int  How many units are skipped each time the window moves.\n",
    "    \"\"\"\n",
    "\n",
    "    text = replace_whitespaces(text)\n",
    "    \n",
    "    if window_stride is None:\n",
    "        window_stride = window_size\n",
    "    \n",
    "    if window_size < window_stride:\n",
    "        print(f'Window size ({window_size}) is smaller than stride length ({window_stride}). This will result in missing chunks of text.')\n",
    "\n",
    "        \n",
    "    if window_type == 'sentence':       \n",
    "        text = replace_stops(text)\n",
    "        sentences = sent_tokenize(text)                \n",
    "        return [\n",
    "            ' '.join(sentences[i:i+window_size]) \n",
    "            for i in range(0, len(sentences), window_stride)\n",
    "        ]\n",
    "    \n",
    "    elif window_type == 'character':\n",
    "        return [\n",
    "            text[i:i+window_size] \n",
    "            for i in range(0, len(text), window_stride)\n",
    "        ]\n",
    "\n",
    "    \n",
    "def gen_sliding_window_delimiters(post_lengths: List[int], max_size: int) -> Generator[Tuple[int, int], None, None]:\n",
    "    \"\"\"\n",
    "    Calculate where to split the sequence of `post_lenghts` so that the individual batches do not exceed `max_size`\n",
    "    \"\"\"\n",
    "    range_length = start = cur_sum = 0\n",
    "    \n",
    "    for post_length in post_lengths:\n",
    "        if (range_length + post_length) > max_size: # exceeds memory\n",
    "            yield (start, start + range_length)\n",
    "            start = cur_sum\n",
    "            range_length = post_length\n",
    "        else: # memory still avail in current split\n",
    "            range_length += post_length\n",
    "        cur_sum += post_length\n",
    "        \n",
    "    if range_length > 0:\n",
    "        yield (start, start + range_length)\n",
    "\n",
    "\n",
    "@result_generator\n",
    "def embedding_results(\n",
    "    dataset: Dataset,\n",
    "    vectorizer_fact_check: Vectorizer,\n",
    "    vectorizer_post: Vectorizer,\n",
    "    sliding_window: bool = False,\n",
    "    sliding_window_pooling: str = 'max',\n",
    "    sliding_window_size: int = None,\n",
    "    sliding_window_stride: int = None,\n",
    "    sliding_window_type: str = None,\n",
    "    post_split_size: int = 256,\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    device: str = 'cpu',\n",
    "    save_if_missing: bool = False\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate results using cosine similarity based on embeddings generated via vectorizers.\n",
    "    \n",
    "    Attributes:\n",
    "        dataset: Dataset\n",
    "        vectorizer_fact_check: Vectorizer  Vectorizer used to process fact-checks\n",
    "        vectorizer_post: Vectorizer  Vectorizer used to process posts\n",
    "        sliding_window: bool  Should sliding window be used or should texts be process without slicing.\n",
    "        sliding_window_pooling: str  One of 'sum', 'mul', 'mean', 'min', 'max' as defined here: https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html\n",
    "        sliding_window_size, sliding_window_stride, sliding_window_type:  See `slice_text`\n",
    "        post_split_size: int  Batch size for post embeddings for sim calculation\n",
    "        dtype: torch.dtype  Data type in which calculate sim\n",
    "        device: str  Device on which calculate sim\n",
    "        save_if_missing: bool  Should the vectors in `dict` be saved after new vectors are calculated? This makes sense for models that will\n",
    "                be used more than once.\n",
    "    \"\"\"\n",
    "        \n",
    "    print('Calculating embeddings for fact checks')\n",
    "    fact_check_embeddings = vectorizer_fact_check.vectorize(\n",
    "        dataset.id_to_fact_check.values(),\n",
    "        save_if_missing=save_if_missing,\n",
    "        normalize=True\n",
    "    )\n",
    "    fact_check_embeddings = fact_check_embeddings.transpose(0, 1)  # Rotate for matmul\n",
    "\n",
    "    fact_check_embeddings = fact_check_embeddings.to(device=device, dtype=dtype)\n",
    "\n",
    "        \n",
    "    # We need to split the calculations because of memory limitations, sims matrix alone requires 200k x 25k x 4 = ~20GB RAM \n",
    "    # memory = 2**30 # assume 4gb free memory - 2**31 = 2gb for both `sims` and `sorted_ids`\n",
    "    # post_split_size = memory // len(dataset.id_to_fact_check) // 4  # // 4 because of float32\n",
    "    post_ids = iter(dataset.id_to_post.keys())\n",
    "    \n",
    "    if sliding_window:\n",
    "        \n",
    "        print('Splitting posts into windows.')\n",
    "        windows = [\n",
    "            slice_text(post, sliding_window_type, sliding_window_size, sliding_window_stride)\n",
    "            for post in dataset.id_to_post.values()\n",
    "        ]\n",
    "\n",
    "        print('Calculating embeddings for the windows')\n",
    "        post_embeddings = vectorizer_post.vectorize(\n",
    "            list(itertools.chain(*windows)),\n",
    "            save_if_missing=save_if_missing,\n",
    "            normalize=True\n",
    "        ) \n",
    "        \n",
    "        # We need to split the matrix matmul so that all the windows from each post belong to the same batch.\n",
    "        post_lengths = [len(post) for post in windows]\n",
    "        segment_array = torch.tensor([\n",
    "            i \n",
    "            for i, num_windows in enumerate(post_lengths) \n",
    "            for _ in range(num_windows)\n",
    "        ])\n",
    "        delimiters = list(gen_sliding_window_delimiters(post_lengths, post_split_size))\n",
    "            \n",
    "        print('Calculating similarity for data splits')\n",
    "        \n",
    "        for start_id, end_id in delimiters:\n",
    "\n",
    "            sims = torch.mm(\n",
    "                post_embeddings[start_id:end_id].to(device=device, dtype=dtype), \n",
    "                fact_check_embeddings\n",
    "            )\n",
    "\n",
    "            segments = segment_array[start_id:end_id]\n",
    "            segments -= int(segments[0])\n",
    "\n",
    "            sims = scatter(\n",
    "                src=sims,\n",
    "                index=segments,\n",
    "                dim=0,\n",
    "                reduce=sliding_window_pooling,\n",
    "            )\n",
    "\n",
    "            sorted_ids = torch.argsort(sims, descending=True, dim=1)\n",
    "\n",
    "            fact_check_ids = {i: fc_id for i, fc_id in enumerate(dataset.id_to_fact_check.keys())}\n",
    "            for row in sorted_ids:\n",
    "                row = row.cpu().numpy()\n",
    "                row = np.vectorize(fact_check_ids.__getitem__)(row)\n",
    "                yield row, next(post_ids)\n",
    "\n",
    "          \n",
    "    else:\n",
    "        \n",
    "        print('Calculating embeddings for posts')\n",
    "        post_embeddings = vectorizer_post.vectorize(\n",
    "            dataset.id_to_post.values(),\n",
    "            save_if_missing=save_if_missing,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        print('Calculating similarity for data splits')\n",
    "        for start_id in range(0, len(dataset.id_to_post), post_split_size):\n",
    "            end_id = start_id + post_split_size\n",
    "\n",
    "            sims = torch.mm(\n",
    "                post_embeddings[start_id:end_id].to(device=device, dtype=dtype), \n",
    "                fact_check_embeddings\n",
    "            )\n",
    "\n",
    "            # TODO: argsort does not duplicities into account, the results might not be deterministic\n",
    "            sorted_ids = torch.argsort(sims, descending=True, dim=1)\n",
    "\n",
    "            fact_check_ids = {i: fc_id for i, fc_id in enumerate(dataset.id_to_fact_check.keys())}\n",
    "            for row in sorted_ids:\n",
    "                row = row.cpu().numpy()\n",
    "                row = np.vectorize(fact_check_ids.__getitem__)(row)\n",
    "                yield row, next(post_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcb5bca1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.512692Z",
     "iopub.status.busy": "2024-12-24T21:35:00.512210Z",
     "iopub.status.idle": "2024-12-24T21:35:00.517529Z",
     "shell.execute_reply": "2024-12-24T21:35:00.516773Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.014395,
     "end_time": "2024-12-24T21:35:00.519034",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.504639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentenceTransformerVectorizer(Vectorizer):\n",
    "    \"\"\"\n",
    "    Vectorizer for `SentenceTransformer` models compatible with `sentence_transformers` library.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dir_path: str,\n",
    "        model_handle: str = None,\n",
    "        model: SentenceTransformer = None,\n",
    "        batch_size: int = 32\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            dir_path: str  Path to cached vectors and vocab files.\n",
    "            model_handle: str  Name of the model, either a HuggingFace repository handle or path to a local model.\n",
    "            model: SentenceTransformer  A loaded model -- this option can be used during fine-tuning.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(dir_path)\n",
    "        \n",
    "        if model_handle:\n",
    "            self.model = SentenceTransformer(model_handle)\n",
    "        else:\n",
    "            self.model = model\n",
    "            \n",
    "        self.batch_size = batch_size\n",
    "            \n",
    "        assert self.model\n",
    "        \n",
    "        \n",
    "    def _calculate_vectors(self, texts: List[str]) -> torch.tensor:\n",
    "        \n",
    "        return self.model.encode(\n",
    "            texts,\n",
    "            batch_size=self.batch_size,\n",
    "            convert_to_numpy=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "918a8f79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.533783Z",
     "iopub.status.busy": "2024-12-24T21:35:00.533527Z",
     "iopub.status.idle": "2024-12-24T21:35:00.541174Z",
     "shell.execute_reply": "2024-12-24T21:35:00.540443Z"
    },
    "papermill": {
     "duration": 0.016779,
     "end_time": "2024-12-24T21:35:00.542701",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.525922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PytorchVectorizer(Vectorizer):\n",
    "    \"\"\"\n",
    "    Vectorizer for `Pytorch` models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dir_path: str,\n",
    "        model_handle: str = None,\n",
    "        model: torch.nn.Module = None,\n",
    "        tokenizer = None,\n",
    "        batch_size: int = 32,\n",
    "        dtype: torch.dtype = torch.float32,\n",
    "        port_embeddings_to_cpu: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            dir_path: str  Path to cached vectors and vocab files.\n",
    "            model_handle: str  Name of the model, either a HuggingFace repository handle or path to a local model.\n",
    "            model: SentenceTransformer  A loaded model -- this option can be used during fine-tuning.\n",
    "            tokenizer: AutoTokenizer  A tokenizer for the model.\n",
    "            batch_size: int  Batch size for inference\n",
    "            dtype: torch.dtype  Inference dtype\n",
    "            port_embeddings_to_cpu: bool  Whether to move the embeddings to CPU after inference.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(dir_path)\n",
    "        \n",
    "        if model_handle:\n",
    "            self.model = torch.load(model_handle)\n",
    "            self.model.eval()        \n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "        assert self.model\n",
    "\n",
    "        self.device = next(self.model.parameters()).device.type\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.dtype = dtype\n",
    "        self.port_embeddings_to_cpu = port_embeddings_to_cpu\n",
    "\n",
    "        \n",
    "    def _calculate_vectors(self, texts: List[str]) -> torch.tensor:\n",
    "\n",
    "        @torch.autocast(device_type=self.device.split(':')[0], dtype=self.dtype)\n",
    "        @torch.no_grad()\n",
    "        def embedding_pipeline(text: List[str], tokenizer, model, device, max_length = 512):\n",
    "            tokenized = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors='pt').to(device)\n",
    "            embeddings = model(**tokenized)\n",
    "            return embeddings.cpu() if self.port_embeddings_to_cpu else embeddings\n",
    "\n",
    "        return torch.vstack(\n",
    "                [\n",
    "                    embedding_pipeline(\n",
    "                        texts[i:i+self.batch_size], \n",
    "                        self.tokenizer, \n",
    "                        self.model, \n",
    "                        device=self.device, \n",
    "                        max_length=512\n",
    "                    ) \n",
    "                    for i in range(0, len(texts), self.batch_size)\n",
    "                ]\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f673ad5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.557184Z",
     "iopub.status.busy": "2024-12-24T21:35:00.556970Z",
     "iopub.status.idle": "2024-12-24T21:35:00.561821Z",
     "shell.execute_reply": "2024-12-24T21:35:00.561037Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.013902,
     "end_time": "2024-12-24T21:35:00.563296",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.549394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://github.com/mosaicml/composer/blob/dev/composer/algorithms/ema/ema.py\n",
    "Exponential Moving Average (EMA) is a model averaging technique that maintains an \n",
    "exponentially weighted moving average of the model parameters during training. \n",
    "The averaged parameters are used for model evaluation. EMA typically results \n",
    "in less noisy validation metrics over the course of training, and sometimes \n",
    "increased generalization.\n",
    "\"\"\"\n",
    "\n",
    "def compute_ema(model: torch.nn.Module, ema_model: torch.nn.Module, smoothing: float = 0.99):\n",
    "    with torch.no_grad():\n",
    "        model_params = itertools.chain(model.parameters(), model.buffers())\n",
    "        ema_model_params = itertools.chain(ema_model.parameters(), ema_model.buffers())\n",
    "\n",
    "        for ema_param, model_param in zip(ema_model_params, model_params):\n",
    "            model_param = model_param.detach()\n",
    "            ema_param.copy_(ema_param * smoothing + (1. - smoothing) * model_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66f0350e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.577967Z",
     "iopub.status.busy": "2024-12-24T21:35:00.577751Z",
     "iopub.status.idle": "2024-12-24T21:35:00.586288Z",
     "shell.execute_reply": "2024-12-24T21:35:00.585577Z"
    },
    "papermill": {
     "duration": 0.01759,
     "end_time": "2024-12-24T21:35:00.587734",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.570144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://github.com/mosaicml/composer/blob/dev/composer/algorithms/sam/sam.py\n",
    "Sharpness-Aware Minimization (SAM) is an optimization algorithm that minimizes \n",
    "both the loss and the sharpness of the loss. It finds parameters that lie in \n",
    "a neighborhood of low loss. The authors find that this improves model generalization\n",
    "\"\"\"\n",
    "\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    \"\"\"Wraps an optimizer with sharpness-aware minimization (`Foret et al, 2020 <https://arxiv.org/abs/2010.01412>`_).\n",
    "    See :class:`.SAM` for details.\n",
    "    Implementation based on https://github.com/davda54/sam\n",
    "    Args:\n",
    "        base_optimizer (torch.optim.Optimizer) The optimizer to apply SAM to.\n",
    "        rho (float, optional): The SAM neighborhood size. Must be greater than 0. Default: ``0.05``.\n",
    "        epsilon (float, optional): A small value added to the gradient norm for numerical stability. Default: ``1.0e-12``.\n",
    "        interval (int, optional): SAM will run once per ``interval`` steps. A value of 1 will\n",
    "            cause SAM to run every step. Steps on which SAM runs take\n",
    "            roughly twice as much time to complete. Default: ``1``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_optimizer: torch.optim.Optimizer,\n",
    "        rho: float = 0.05,\n",
    "        epsilon: float = 1.0e-12,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if rho < 0:\n",
    "            raise ValueError(f'Invalid rho, should be non-negative: {rho}')\n",
    "        self.base_optimizer = base_optimizer\n",
    "        defaults = {'rho': rho, 'epsilon': epsilon, **kwargs}\n",
    "        super(SAM, self).__init__(self.base_optimizer.param_groups, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group['rho'] / (grad_norm + group['epsilon'])\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                e_w = p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                self.state[p]['e_w'] = e_w\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None or 'e_w' not in self.state[p]:\n",
    "                    continue\n",
    "                p.sub_(self.state[p]['e_w'])  # get back to \"w\" from \"w + e(w)\"\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, step_type: str):\n",
    "        \"\"\" Adjusted to PyTorch mixed precision training framework\n",
    "        \"\"\"\n",
    "        if step_type == 'first':\n",
    "            self.first_step()\n",
    "        elif step_type == 'second':\n",
    "            self.second_step()\n",
    "        elif step_type == 'skip':\n",
    "            self.base_optimizer.step()\n",
    "\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        norm = torch.norm(torch.stack(\n",
    "            [p.grad.norm(p=2) for group in self.param_groups for p in group['params'] if p.grad is not None]),\n",
    "                          p='fro')\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "233a9502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.603091Z",
     "iopub.status.busy": "2024-12-24T21:35:00.602873Z",
     "iopub.status.idle": "2024-12-24T21:35:00.614080Z",
     "shell.execute_reply": "2024-12-24T21:35:00.613436Z"
    },
    "papermill": {
     "duration": 0.020643,
     "end_time": "2024-12-24T21:35:00.615524",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.594881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://github.com/mosaicml/composer/blob/dev/composer/algorithms/layer_freezing/layer_freezing.py\n",
    "Layer Freezing gradually makes early modules untrainable (\"freezing\" them), saving the cost of \n",
    "backpropagating to and updating frozen modules. The hypothesis behind Layer Freezing \n",
    "is that early layers may learn their features sooner than later layers, meaning they \n",
    "do not need to be updated later in training. Especially for fine-tuning, which is our case.\n",
    "\"\"\"\n",
    "\n",
    "def freeze_layers(\n",
    "    model: torch.nn.Module,\n",
    "    optimizers: Union[Optimizer, Sequence[Optimizer]],\n",
    "    current_duration: float,\n",
    "    freeze_start: float = 0.5,\n",
    "    freeze_level: float = 1.0,\n",
    ") -> Tuple[int, float]:\n",
    "    \"\"\"Progressively freeze the layers of the network in-place\n",
    "    during training, starting with the earlier layers.\n",
    "    Example:\n",
    "         .. testcode::\n",
    "            from composer.algorithms.layer_freezing import freeze_layers\n",
    "            freeze_depth, feeze_level = freeze_layers(\n",
    "                                            model=model,\n",
    "                                            optimizers=optimizer,\n",
    "                                            current_duration=0.5,\n",
    "                                            freeze_start=0.0,\n",
    "                                            freeze_level=1.0\n",
    "                                        )\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model being trained.\n",
    "        optimizers (torch.optim.Optimizer | Sequence[torch.optim.Optimizer]):\n",
    "            The optimizers used during training.\n",
    "        current_duration (float): The fraction, in ``[0, 1)`` of the training process complete.\n",
    "        freeze_start (float, optional): The fraction of the training process in ``[0, 1)`` to run\n",
    "            before freezing begins. Default: ``0.5``.\n",
    "        freeze_level (float, optional): The maximum fraction of layers on ``[0, 1)`` to freeze.\n",
    "            Default: ``1.0``.\n",
    "    Return:\n",
    "        (int, float): The number of layers frozen, and the percentage of the total model frozen.\n",
    "    \"\"\"\n",
    "    # Flatten out the layers\n",
    "    flat_children = []\n",
    "    _get_layers(model, flat_children)\n",
    "    # Determine how many layers to freeze\n",
    "    freeze_percentage = _freeze_schedule(current_duration=current_duration,\n",
    "                                         freeze_start=freeze_start,\n",
    "                                         freeze_level=freeze_level)\n",
    "    freeze_depth = int(freeze_percentage * len(flat_children[0:-1]))\n",
    "\n",
    "    # Freeze the parameters in the chosen layers\n",
    "    for i, child in enumerate(flat_children[0:-1]):\n",
    "        if i < freeze_depth:\n",
    "            for p in child.parameters():\n",
    "                _remove_param_from_optimizers(p, optimizers)\n",
    "                # Do not compute gradients for this param.\n",
    "                p.requires_grad = False\n",
    "\n",
    "    return freeze_depth, freeze_percentage\n",
    "\n",
    "\n",
    "def _freeze_schedule(current_duration: float, freeze_start: float, freeze_level: float) -> float:\n",
    "    \"\"\"Implements a linear schedule for freezing.\n",
    "    The schedule is linear and begins with no freezing and linearly\n",
    "    increases the fraction of layers frozen, reaching the fraction specified by ``freeze_level`` at the end of training.\n",
    "    The start of freezing is given as a fraction of the total training duration and is set with ``freeze_start``.\n",
    "    Args:\n",
    "        current_duration (float): The elapsed training duration.\n",
    "        freeze_start (float): The fraction of training to run before freezing begins.\n",
    "        freeze_level (float): The maximum fraction of levels to freeze.\n",
    "    \"\"\"\n",
    "    # No freezing if the current epoch is less than this\n",
    "    if current_duration <= freeze_start:\n",
    "        return 0.0\n",
    "    # `Calculate the total time for freezing to occur\n",
    "    total_freezing_time = 1.0 - freeze_start\n",
    "    # Calculate the amount of freezing time that has elapsed\n",
    "    freezing_time_elapsed = current_duration - freeze_start\n",
    "    # Calculate the fraction of the freezing time elapsed.\n",
    "    freezing_time_elapsed_frac = freezing_time_elapsed / total_freezing_time\n",
    "    # Scale this fraction by the amount of freezing to do.\n",
    "    return freeze_level * freezing_time_elapsed_frac\n",
    "\n",
    "\n",
    "def _get_layers(module: torch.nn.Module, flat_children: List[torch.nn.Module]):\n",
    "    \"\"\"Helper function to get all submodules.\n",
    "    Does a depth first search to flatten out modules which\n",
    "    contain parameters.\n",
    "    Args:\n",
    "        module (torch.nn.Module): Current module to search.\n",
    "        flat_children (List[torch.nn.Module]): List containing modules.\n",
    "    \"\"\"\n",
    "    # Check if given module has no children and parameters.\n",
    "    if (len(list(module.children())) == 0 and len(list(module.parameters())) > 0):\n",
    "        flat_children.append(module)\n",
    "    else:\n",
    "        # Otherwise, continue the search over its children.\n",
    "        for child in module.children():\n",
    "            _get_layers(child, flat_children)\n",
    "\n",
    "\n",
    "def _remove_param_from_optimizers(p: torch.nn.Parameter, optimizers: Union[Optimizer, Sequence[Optimizer]]):\n",
    "    \"\"\"Helper function to freeze the training of a parameter.\n",
    "    To freeze a parameter, it must be removed from the optimizer,\n",
    "    otherwise momentum and weight decay may still be applied.\n",
    "    Args:\n",
    "        p (torch.nn.Parameter): The parameter being frozen.\n",
    "        optimizers (torch.optim.Optimizer | Sequence[torch.optim.Optimizer]): The optimizers used during training.\n",
    "    \"\"\"\n",
    "    # Search over params in the optimizers to find and remove the\n",
    "    # given param. Necessary due to the way params are stored.\n",
    "    for optimizer in ensure_tuple(optimizers):\n",
    "        for group in optimizer.param_groups:\n",
    "            group['params'] = list(filter(lambda x: id(x) != id(p), group['params']))\n",
    "\n",
    "\n",
    "def ensure_tuple(x):\n",
    "    \"\"\"Converts ``x`` into a tuple.\n",
    "    * If ``x`` is ``None``, then ``tuple()`` is returned.\n",
    "    * If ``x`` is a tuple, then ``x`` is returned as-is.\n",
    "    * If ``x`` is a list, then ``tuple(x)`` is returned.\n",
    "    * If ``x`` is a dict, then ``tuple(v for v in x.values())`` is returned.\n",
    "    Otherwise, a single element tuple of ``(x,)`` is returned.\n",
    "    Args:\n",
    "        x (Any): The input to convert into a tuple.\n",
    "    Returns:\n",
    "        tuple: A tuple of ``x``.\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return ()\n",
    "    if isinstance(x, (str, bytes, bytearray)):\n",
    "        return (x,)\n",
    "    if isinstance(x, collections.abc.Sequence):\n",
    "        return tuple(x)\n",
    "    if isinstance(x, dict):\n",
    "        return tuple(x.values())\n",
    "    return (x,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b691ad3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.630086Z",
     "iopub.status.busy": "2024-12-24T21:35:00.629602Z",
     "iopub.status.idle": "2024-12-24T21:35:00.634112Z",
     "shell.execute_reply": "2024-12-24T21:35:00.633485Z"
    },
    "papermill": {
     "duration": 0.013416,
     "end_time": "2024-12-24T21:35:00.635651",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.622235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MNRloss(nn.Module):\n",
    "    def __init__(self, label_smoothing=0):\n",
    "        super().__init__()\n",
    "        self.loss_f = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "    def forward(self, sentence_embedding_A: torch.Tensor, sentence_embedding_B: torch.Tensor):\n",
    "        # Compute similarity matrix\n",
    "        scores = torch.mm(sentence_embedding_A, sentence_embedding_B.transpose(0, 1))\n",
    "        # Compute labels\n",
    "        labels = torch.arange(len(scores), dtype=torch.long, device=scores.device)\n",
    "        return self.loss_f(scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76bd07a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.650071Z",
     "iopub.status.busy": "2024-12-24T21:35:00.649857Z",
     "iopub.status.idle": "2024-12-24T21:35:00.656357Z",
     "shell.execute_reply": "2024-12-24T21:35:00.655592Z"
    },
    "papermill": {
     "duration": 0.015384,
     "end_time": "2024-12-24T21:35:00.657822",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.642438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name, pooling='default'):\n",
    "        super(Model, self).__init__()\n",
    "        self.pooling = pooling\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        \n",
    "        if isinstance(config, T5Config):         \n",
    "            from transformers import T5EncoderModel\n",
    "            T5EncoderModel._keys_to_ignore_on_load_unexpected = [\"decoder.*\"]\n",
    "            self.model = T5EncoderModel.from_pretrained(model_name, config=config)\n",
    "        else:\n",
    "            self.model = AutoModel.from_pretrained(model_name)\n",
    "            \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        model_output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        if self.pooling == 'default':\n",
    "            return model_output[1]\n",
    "        elif self.pooling == 'mean':       \n",
    "            return self.mean_pooling(model_output[0], attention_mask)\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def mean_pooling(token_embeddings: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    \n",
    "def get_model_tokenizer(model_name: str, **kwargs):\n",
    "    return Model(model_name, **kwargs), AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca2f6127",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.672204Z",
     "iopub.status.busy": "2024-12-24T21:35:00.671970Z",
     "iopub.status.idle": "2024-12-24T21:35:00.677778Z",
     "shell.execute_reply": "2024-12-24T21:35:00.677127Z"
    },
    "papermill": {
     "duration": 0.01476,
     "end_time": "2024-12-24T21:35:00.679330",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.664570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_opimizer_scheduler(model, cfg: Config):\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)]},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    \n",
    "    optimizer = _get_optimizer(optimizer_grouped_parameters, cfg)\n",
    "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=cfg.train.warmup_steps,\n",
    "        num_training_steps=cfg.train.num_steps,\n",
    "        num_cycles=1.0    \n",
    "    )\n",
    "    \n",
    "    if cfg.optimizer.sam:\n",
    "        return SAM(optimizer, rho=cfg.optimizer.sam_rho)\n",
    "    else:\n",
    "        return optimizer, scheduler\n",
    "\n",
    "\n",
    "def _get_optimizer(parameters, cfg: Config):\n",
    "    if cfg.optimizer.name == 'adamw':\n",
    "        return torch.optim.AdamW(\n",
    "            parameters, \n",
    "            lr=cfg.optimizer.lr,\n",
    "            weight_decay=cfg.optimizer.weight_decay\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(f'Wrong optimizer name {cfg.optimizer.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea78c23d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.693839Z",
     "iopub.status.busy": "2024-12-24T21:35:00.693625Z",
     "iopub.status.idle": "2024-12-24T21:35:00.712681Z",
     "shell.execute_reply": "2024-12-24T21:35:00.712021Z"
    },
    "papermill": {
     "duration": 0.028178,
     "end_time": "2024-12-24T21:35:00.714224",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.686046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH = Tuple[Dict[str, torch.Tensor]]\n",
    "\n",
    "\n",
    "def safe_mean(x, round_to=4):\n",
    "    try:\n",
    "        return round(mean(x), round_to)\n",
    "    except StatisticsError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def collate_fn(batch, tokenizer, cfg: Config):\n",
    "    p_text, fc_text = zip(*batch)\n",
    "    return (\n",
    "        tokenizer(list(p_text), padding=True, truncation=True, max_length=cfg.train.p_max_seq_length, return_tensors='pt'),\n",
    "        tokenizer(list(fc_text), padding=True, truncation=True, max_length=cfg.train.fc_max_seq_length, return_tensors='pt')\n",
    "    )\n",
    "\n",
    "\n",
    "def train_step(step, model, batch: BATCH, loss_fn, optimizer, scheduler, scaler, ctx_autocast, cfg: Config):\n",
    "    model.train()\n",
    "    posts_encoded, fact_checks_encoded = batch\n",
    "    posts_encoded, fact_checks_encoded = posts_encoded.to(cfg.train.device), fact_checks_encoded.to(cfg.train.device)\n",
    "    \n",
    "    def fw_bw():\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with ctx_autocast:\n",
    "            loss = loss_fn(model(**fact_checks_encoded), model(**posts_encoded))\n",
    "        scaler.scale(loss).backward()\n",
    "        if cfg.optimizer.clip_value is not None:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.optimizer.clip_value)\n",
    "        return loss  \n",
    "      \n",
    "    if isinstance(optimizer, SAM):\n",
    "        step_type = ('first', 'second') if (step % cfg.optimizer.sam_step) == 0 else ('skip',)\n",
    "        for s in step_type:\n",
    "            loss = fw_bw()\n",
    "            scaler.step(optimizer, step_type=s)\n",
    "            scaler.update()\n",
    "    else:\n",
    "        loss = fw_bw()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    scheduler.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def evaluate_datasets(datasets, model, tokenizer, run_path, save_vectors):\n",
    "    model.eval()\n",
    "    results = {}\n",
    "        \n",
    "    vectorizer_path = run_path if save_vectors else None\n",
    "    vct = PytorchVectorizer(dir_path=vectorizer_path, model=model, tokenizer=tokenizer, batch_size=256, port_embeddings_to_cpu=True)\n",
    "    \n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        result_gen = embedding_results(dataset, vct, vct, post_split_size=512, device='cuda')\n",
    "        metrics = process_result_generator(result_gen, csv_path=os.path.join(run_path, dataset_name + '.csv'))\n",
    "        for value_name, values in metrics.items():\n",
    "            v_mean, v_ci_lower, v_ci_upper = values\n",
    "            results[f'{dataset_name}_{value_name}_mean'] = v_mean\n",
    "            results[f'{dataset_name}_{value_name}_lower'] = v_ci_lower \n",
    "            results[f'{dataset_name}_{value_name}_upper'] = v_ci_upper  \n",
    "    return results\n",
    "\n",
    "\n",
    "def train(cfg, train_dataset, dev_datasets, test_datasets = None):\n",
    "\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    if cfg.train.output_path is not None:\n",
    "        run_path = os.path.join(cfg.train.output_path, cfg.timestamp)\n",
    "        os.makedirs(run_path)\n",
    "        # save config\n",
    "        with open(os.path.join(run_path, 'config.yaml') ,'w') as f:\n",
    "            yaml.dump(cfg.to_dict(), f, default_flow_style=False)\n",
    "\n",
    "    print('Get model')\n",
    "    model, tokenizer = get_model_tokenizer(cfg.model.name, pooling=cfg.model.pooling)\n",
    "    model = model.to(cfg.train.device)\n",
    "\n",
    "    # Only if EMA is used\n",
    "    if cfg.train.ema:\n",
    "        ema_model = copy.deepcopy(model)\n",
    "\n",
    "    print('Prepare dataloader')\n",
    "    partial_collate_fn = partial(collate_fn, tokenizer=tokenizer, cfg=cfg)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=cfg.train.batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=partial_collate_fn, \n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    cfg.train.num_steps = len(train_dataloader) * cfg.train.num_epochs\n",
    "    # If evaluation is done after each epoch\n",
    "    if cfg.train.evaluation_steps == -1:\n",
    "        cfg.train.evaluation_steps = len(train_dataloader)\n",
    "\n",
    "    print('Prepare optimizer and loss')\n",
    "    optimizer, scheduler = get_opimizer_scheduler(model, cfg)\n",
    "    loss_fn = MNRloss(cfg.train.label_smoothing)\n",
    "    \n",
    "    print('Train')\n",
    "    step = 0\n",
    "    training_loss = []\n",
    "\n",
    "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[cfg.train.dtype]\n",
    "    ctx_autocast = torch.autocast(device_type=cfg.train.device.split(':')[0], dtype=ptdtype)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(cfg.train.dtype == 'float16'))\n",
    "\n",
    "    for epoch in range(cfg.train.num_epochs):\n",
    "        print(f\"EPOCH: {epoch}\")\n",
    "        for batch in train_dataloader:\n",
    "            step += 1\n",
    "\n",
    "            train_loss_value = train_step(step, model, batch, loss_fn, optimizer, scheduler, scaler, ctx_autocast, cfg)\n",
    "            training_loss.append(train_loss_value)\n",
    "            \n",
    "            # Loss reporting\n",
    "            if (step % cfg.train.metrics_window_size) == 0: \n",
    "                agg_loss = mean(training_loss)\n",
    "                print(\n",
    "                    f'step {step}/{cfg.train.num_steps}: train/loss-mean{cfg.train.metrics_window_size} {agg_loss:.4f}'\n",
    "                )\n",
    "                print({\n",
    "                    f'train/loss-last-step{cfg.train.metrics_window_size}': train_loss_value, \n",
    "                    'lr': optimizer.param_groups[0]['lr']\n",
    "                    }\n",
    "                )\n",
    "                training_loss = []\n",
    "\n",
    "            # Only if EMA is used\n",
    "            if cfg.train.ema:\n",
    "                compute_ema(model, ema_model, smoothing=0.99)\n",
    "            \n",
    "            # Evaluation reporting\n",
    "            if (step % cfg.train.evaluation_steps == 0) or (cfg.train.num_steps == step):\n",
    "                step_path = os.path.join(run_path, str(step))\n",
    "                os.makedirs(step_path)\n",
    "\n",
    "                eval_model = ema_model if cfg.train.ema else model\n",
    "                # results = evaluate_datasets({**dev_datasets, **test_datasets}, eval_model, tokenizer, step_path, save_vectors=cfg.train.save_vectors)\n",
    "                results = evaluate_datasets({**dev_datasets}, eval_model, tokenizer, step_path, save_vectors=cfg.train.save_vectors)\n",
    "\n",
    "                mean_dev_ps = safe_mean([results[f'{dataset_name}_pair_success_at_10_mean'] for dataset_name in dev_datasets.keys()])\n",
    "                # mean_train_ps = safe_mean([results[f'{dataset_name}_pair_success_at_10_mean'] for dataset_name in test_datasets.keys()])\n",
    "                print(\n",
    "                    f'step {step}/{cfg.train.num_steps}: dev/mean-ps@10 {mean_dev_ps}'\n",
    "                )\n",
    "\n",
    "                if cfg.train.save_models:\n",
    "                    torch.save(eval_model.state_dict(), os.path.join(step_path, 'model.pt'))\n",
    "\n",
    "        # Applying layer freezing at the end of the epoch\n",
    "        if cfg.train.freezing:\n",
    "            _, feeze_level = freeze_layers(\n",
    "                model=model, optimizers=optimizer, \n",
    "                current_duration=epoch / cfg.train.num_epochs, \n",
    "                freeze_start=0.0, freeze_level=1.0\n",
    "            )\n",
    "            print({'step': step, 'feeze_level': feeze_level})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f86aeab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T21:35:00.729050Z",
     "iopub.status.busy": "2024-12-24T21:35:00.728831Z",
     "iopub.status.idle": "2024-12-25T02:09:39.375021Z",
     "shell.execute_reply": "2024-12-25T02:09:39.374020Z"
    },
    "papermill": {
     "duration": 16478.656046,
     "end_time": "2024-12-25T02:09:39.377177",
     "exception": false,
     "start_time": "2024-12-24T21:35:00.721131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fact-checks.\n",
      "153743 loaded.\n",
      "Loading posts.\n",
      "24431 loaded.\n",
      "Loading fact-check-post mapping.\n",
      "25743 loaded.\n",
      "Filtering by split: 17590 posts remaining and sampled 44223 fact checks\n",
      "Filtering fact-checks by language: 44223 posts remaining.\n",
      "Filtering posts by language: 17590 posts remaining.\n",
      "Mappings remaining: 20594.\n",
      "Filtering posts.\n",
      "Posts remaining: 17590\n",
      "Filtering by split: 4398 posts remaining and sampled 44223 fact checks\n",
      "Filtering fact-checks by language: 44223 posts remaining.\n",
      "Filtering posts by language: 4398 posts remaining.\n",
      "Mappings remaining: 5149.\n",
      "Filtering posts.\n",
      "Posts remaining: 4398\n",
      "Get model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f9ba243b3c4e309d215f9a41c8882e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ccba93fb7644c19605b2825f9df56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99406d6d32a943f599cdf9c29c37e4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d890c876f64f00bfbf7c235e43787d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8e231ece744d97817a5d3b34f6187a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d7ac41d5ad486b9564428aef76edb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare dataloader\n",
      "Prepare optimizer and loss\n",
      "Train\n",
      "EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/3492125157.py:117: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(cfg.train.dtype == 'float16'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 128/12870: train/loss-mean128 3.0170\n",
      "{'train/loss-last-step128': 9.685717259344528e-07, 'lr': 1.6000000000000001e-06}\n",
      "step 256/12870: train/loss-mean128 0.5507\n",
      "{'train/loss-last-step128': 0.0, 'lr': 3.2000000000000003e-06}\n",
      "step 384/12870: train/loss-mean128 0.2481\n",
      "{'train/loss-last-step128': 0.0, 'lr': 4.800000000000001e-06}\n",
      "step 512/12870: train/loss-mean128 0.2319\n",
      "{'train/loss-last-step128': 2.9280853271484375, 'lr': 4.999004860230925e-06}\n",
      "step 640/12870: train/loss-mean128 0.1453\n",
      "{'train/loss-last-step128': 1.370440125465393, 'lr': 4.995431569514878e-06}\n",
      "step 768/12870: train/loss-mean128 0.1941\n",
      "{'train/loss-last-step128': 0.0, 'lr': 4.989263533058938e-06}\n",
      "step 896/12870: train/loss-mean128 0.1804\n",
      "{'train/loss-last-step128': 1.3749001026153564, 'lr': 4.980507164377509e-06}\n",
      "step 1024/12870: train/loss-mean128 0.1583\n",
      "{'train/loss-last-step128': 0.06668083369731903, 'lr': 4.969171568328689e-06}\n",
      "step 1152/12870: train/loss-mean128 0.1198\n",
      "{'train/loss-last-step128': 1.4901160305669237e-08, 'lr': 4.955268531647052e-06}\n",
      "step 1280/12870: train/loss-mean128 0.1662\n",
      "{'train/loss-last-step128': 5.474977660924196e-05, 'lr': 4.9388125106878195e-06}\n",
      "step 1408/12870: train/loss-mean128 0.0943\n",
      "{'train/loss-last-step128': 0.0002985313185490668, 'lr': 4.919820616395166e-06}\n",
      "step 1536/12870: train/loss-mean128 0.2634\n",
      "{'train/loss-last-step128': 1.4901160305669237e-08, 'lr': 4.8983125965102905e-06}\n",
      "step 1664/12870: train/loss-mean128 0.1425\n",
      "{'train/loss-last-step128': 0.0, 'lr': 4.8743108150377396e-06}\n",
      "step 1792/12870: train/loss-mean128 0.2337\n",
      "{'train/loss-last-step128': 0.0, 'lr': 4.8478402289913566e-06}\n",
      "step 1920/12870: train/loss-mean128 0.1215\n",
      "{'train/loss-last-step128': 4.470347647611561e-08, 'lr': 4.818928362444017e-06}\n",
      "step 2048/12870: train/loss-mean128 0.1665\n",
      "{'train/loss-last-step128': 1.4230903387069702, 'lr': 4.787605277908145e-06}\n",
      "step 2176/12870: train/loss-mean128 0.1032\n",
      "{'train/loss-last-step128': 0.21511341631412506, 'lr': 4.753903545076753e-06}\n",
      "step 2304/12870: train/loss-mean128 0.0984\n",
      "{'train/loss-last-step128': 0.0, 'lr': 4.717858206957536e-06}\n",
      "step 2432/12870: train/loss-mean128 0.1074\n",
      "{'train/loss-last-step128': 0.0, 'lr': 4.679506743435206e-06}\n",
      "step 2560/12870: train/loss-mean128 0.1766\n",
      "{'train/loss-last-step128': 0.0, 'lr': 4.638889032299964e-06}\n",
      "Calculating embeddings for fact checks\n",
      "Calculating 44221 vectors.\n",
      "Calculating embeddings for posts\n",
      "Calculating 4389 vectors.\n",
      "Calculating similarity for data splits\n",
      "6 ranks produced.\n",
      "step 2574/12870: dev/mean-ps@10 0.1952\n",
      "EPOCH: 1\n",
      "step 2688/12870: train/loss-mean128 0.1071\n",
      "{'train/loss-last-step128': 8.091068593785167e-06, 'lr': 4.596047307782641e-06}\n",
      "step 2816/12870: train/loss-mean128 0.0513\n",
      "{'train/loss-last-step128': 0.0, 'lr': 4.551026116639615e-06}\n",
      "step 2944/12870: train/loss-mean128 0.0442\n",
      "{'train/loss-last-step128': 0.0, 'lr': 4.50387227183316e-06}\n",
      "step 3072/12870: train/loss-mean128 0.0291\n",
      "{'train/loss-last-step128': 0.0, 'lr': 4.454634803855407e-06}\n",
      "step 3200/12870: train/loss-mean128 0.0485\n",
      "{'train/loss-last-step128': 4.7495876060565934e-05, 'lr': 4.403364909746516e-06}\n",
      "step 3328/12870: train/loss-mean128 0.0527\n",
      "{'train/loss-last-step128': 1.4901160305669237e-08, 'lr': 4.350115899860072e-06}\n",
      "step 3456/12870: train/loss-mean128 0.0373\n",
      "{'train/loss-last-step128': 0.0, 'lr': 4.294943142431071e-06}\n",
      "step 3584/12870: train/loss-mean128 0.0384\n",
      "{'train/loss-last-step128': 0.0, 'lr': 4.2379040060041175e-06}\n",
      "step 3712/12870: train/loss-mean128 0.0507\n",
      "{'train/loss-last-step128': 0.0, 'lr': 4.1790577997816965e-06}\n",
      "step 3840/12870: train/loss-mean128 0.0032\n",
      "{'train/loss-last-step128': 0.0, 'lr': 4.11846571195457e-06}\n",
      "step 3968/12870: train/loss-mean128 0.0744\n",
      "{'train/loss-last-step128': 0.0, 'lr': 4.0561907460783865e-06}\n",
      "step 4096/12870: train/loss-mean128 0.0936\n",
      "{'train/loss-last-step128': 7.226854449982056e-06, 'lr': 3.992297655562691e-06}\n",
      "step 4224/12870: train/loss-mean128 0.0337\n",
      "{'train/loss-last-step128': 0.0, 'lr': 3.92685287634043e-06}\n",
      "step 4352/12870: train/loss-mean128 0.0356\n",
      "{'train/loss-last-step128': 0.0, 'lr': 3.859924457787975e-06}\n",
      "step 4480/12870: train/loss-mean128 0.0157\n",
      "{'train/loss-last-step128': 4.25951620854903e-05, 'lr': 3.7915819919674923e-06}\n",
      "step 4608/12870: train/loss-mean128 0.0481\n",
      "{'train/loss-last-step128': 0.007172346580773592, 'lr': 3.721896541265227e-06}\n",
      "step 4736/12870: train/loss-mean128 0.0433\n",
      "{'train/loss-last-step128': 0.24561449885368347, 'lr': 3.6509405645009487e-06}\n",
      "step 4864/12870: train/loss-mean128 0.0602\n",
      "{'train/loss-last-step128': 0.00038935636985115707, 'lr': 3.578787841585386e-06}\n",
      "step 4992/12870: train/loss-mean128 0.0429\n",
      "{'train/loss-last-step128': 0.0, 'lr': 3.505513396803996e-06}\n",
      "step 5120/12870: train/loss-mean128 0.0445\n",
      "{'train/loss-last-step128': 0.0, 'lr': 3.4311934208068282e-06}\n",
      "Calculating embeddings for fact checks\n",
      "Calculating 44221 vectors.\n",
      "Calculating embeddings for posts\n",
      "Calculating 4389 vectors.\n",
      "Calculating similarity for data splits\n",
      "6 ranks produced.\n",
      "step 5148/12870: dev/mean-ps@10 0.1952\n",
      "EPOCH: 2\n",
      "step 5248/12870: train/loss-mean128 0.0140\n",
      "{'train/loss-last-step128': 0.0, 'lr': 3.3559051913856145e-06}\n",
      "step 5376/12870: train/loss-mean128 0.0223\n",
      "{'train/loss-last-step128': 0.0, 'lr': 3.279726993120442e-06}\n",
      "step 5504/12870: train/loss-mean128 0.0501\n",
      "{'train/loss-last-step128': 3.4272636639798293e-07, 'lr': 3.202738035979571e-06}\n",
      "step 5632/12870: train/loss-mean128 0.0237\n",
      "{'train/loss-last-step128': 0.0, 'lr': 3.1250183729570433e-06}\n",
      "step 5760/12870: train/loss-mean128 0.0321\n",
      "{'train/loss-last-step128': 0.0, 'lr': 3.0466488168336988e-06}\n",
      "step 5888/12870: train/loss-mean128 0.0138\n",
      "{'train/loss-last-step128': 0.0, 'lr': 2.967710856148184e-06}\n",
      "step 6016/12870: train/loss-mean128 0.0213\n",
      "{'train/loss-last-step128': 0.0, 'lr': 2.8882865704652952e-06}\n",
      "step 6144/12870: train/loss-mean128 0.0201\n",
      "{'train/loss-last-step128': 0.0, 'lr': 2.8084585450297793e-06}\n",
      "step 6272/12870: train/loss-mean128 0.0091\n",
      "{'train/loss-last-step128': 0.43961620330810547, 'lr': 2.7283097848943314e-06}\n",
      "step 6400/12870: train/loss-mean128 0.0101\n",
      "{'train/loss-last-step128': 0.0, 'lr': 2.647923628611071e-06}\n",
      "step 6528/12870: train/loss-mean128 0.0803\n",
      "{'train/loss-last-step128': 0.0, 'lr': 2.5673836615762536e-06}\n",
      "step 6656/12870: train/loss-mean128 0.0133\n",
      "{'train/loss-last-step128': 0.0, 'lr': 2.4867736291183043e-06}\n",
      "step 6784/12870: train/loss-mean128 0.0102\n",
      "{'train/loss-last-step128': 0.0, 'lr': 2.406177349419565e-06}\n",
      "step 6912/12870: train/loss-mean128 0.0096\n",
      "{'train/loss-last-step128': 0.0, 'lr': 2.325678626362279e-06}\n",
      "step 7040/12870: train/loss-mean128 0.0206\n",
      "{'train/loss-last-step128': 0.0, 'lr': 2.2453611623894514e-06}\n",
      "step 7168/12870: train/loss-mean128 0.0165\n",
      "{'train/loss-last-step128': 0.00045179043081589043, 'lr': 2.1653084714711826e-06}\n",
      "step 7296/12870: train/loss-mean128 0.0368\n",
      "{'train/loss-last-step128': 0.0, 'lr': 2.0856037922669735e-06}\n",
      "step 7424/12870: train/loss-mean128 0.0212\n",
      "{'train/loss-last-step128': 0.0030697088222950697, 'lr': 2.006330001574308e-06}\n",
      "step 7552/12870: train/loss-mean128 0.0175\n",
      "{'train/loss-last-step128': 0.0, 'lr': 1.927569528153482e-06}\n",
      "step 7680/12870: train/loss-mean128 0.0360\n",
      "{'train/loss-last-step128': 0.0, 'lr': 1.8494042670183206e-06}\n",
      "Calculating embeddings for fact checks\n",
      "Calculating 44221 vectors.\n",
      "Calculating embeddings for posts\n",
      "Calculating 4389 vectors.\n",
      "Calculating similarity for data splits\n",
      "6 ranks produced.\n",
      "step 7722/12870: dev/mean-ps@10 0.1952\n",
      "EPOCH: 3\n",
      "step 7808/12870: train/loss-mean128 0.0085\n",
      "{'train/loss-last-step128': 0.0, 'lr': 1.7719154942818672e-06}\n",
      "step 7936/12870: train/loss-mean128 0.0143\n",
      "{'train/loss-last-step128': 0.0, 'lr': 1.6951837826456158e-06}\n",
      "step 8064/12870: train/loss-mean128 0.0055\n",
      "{'train/loss-last-step128': 0.0, 'lr': 1.6192889176201423e-06}\n",
      "step 8192/12870: train/loss-mean128 0.0197\n",
      "{'train/loss-last-step128': 8.940694584680386e-08, 'lr': 1.5443098145642612e-06}\n",
      "step 8320/12870: train/loss-mean128 0.0323\n",
      "{'train/loss-last-step128': 8.791654408923932e-07, 'lr': 1.4703244366289616e-06}\n",
      "step 8448/12870: train/loss-mean128 0.0096\n",
      "{'train/loss-last-step128': 0.0, 'lr': 1.397409713691446e-06}\n",
      "step 8576/12870: train/loss-mean128 0.0057\n",
      "{'train/loss-last-step128': 0.0, 'lr': 1.3256414623635693e-06}\n",
      "step 8704/12870: train/loss-mean128 0.0181\n",
      "{'train/loss-last-step128': 0.0, 'lr': 1.2550943071578497e-06}\n",
      "step 8832/12870: train/loss-mean128 0.0022\n",
      "{'train/loss-last-step128': 0.0, 'lr': 1.1858416028930142e-06}\n",
      "step 8960/12870: train/loss-mean128 0.0252\n",
      "{'train/loss-last-step128': 0.0, 'lr': 1.117955358419781e-06}\n",
      "step 9088/12870: train/loss-mean128 0.0101\n",
      "{'train/loss-last-step128': 0.0, 'lr': 1.0515061617461725e-06}\n",
      "step 9216/12870: train/loss-mean128 0.0030\n",
      "{'train/loss-last-step128': 0.0, 'lr': 9.865631066402138e-07}\n",
      "step 9344/12870: train/loss-mean128 0.0001\n",
      "{'train/loss-last-step128': 0.0, 'lr': 9.231937207863459e-07}\n",
      "step 9472/12870: train/loss-mean128 0.0228\n",
      "{'train/loss-last-step128': 0.0, 'lr': 8.614638955702484e-07}\n",
      "step 9600/12870: train/loss-mean128 0.0007\n",
      "{'train/loss-last-step128': 1.0430808572436945e-07, 'lr': 8.01437817565083e-07}\n",
      "step 9728/12870: train/loss-mean128 0.0208\n",
      "{'train/loss-last-step128': 5.960463056453591e-08, 'lr': 7.431779017903947e-07}\n",
      "step 9856/12870: train/loss-mean128 0.0179\n",
      "{'train/loss-last-step128': 0.0, 'lr': 6.867447268130767e-07}\n",
      "step 9984/12870: train/loss-mean128 0.0022\n",
      "{'train/loss-last-step128': 8.940491534303874e-05, 'lr': 6.32196971757876e-07}\n",
      "step 10112/12870: train/loss-mean128 0.0290\n",
      "{'train/loss-last-step128': 0.0, 'lr': 5.795913552929286e-07}\n",
      "step 10240/12870: train/loss-mean128 0.0071\n",
      "{'train/loss-last-step128': 0.0, 'lr': 5.28982576653784e-07}\n",
      "Calculating embeddings for fact checks\n",
      "Calculating 44221 vectors.\n",
      "Calculating embeddings for posts\n",
      "Calculating 4389 vectors.\n",
      "Calculating similarity for data splits\n",
      "6 ranks produced.\n",
      "step 10296/12870: dev/mean-ps@10 0.1952\n",
      "EPOCH: 4\n",
      "step 10368/12870: train/loss-mean128 0.0111\n",
      "{'train/loss-last-step128': 1.6540179785806686e-06, 'lr': 4.804232587672239e-07}\n",
      "step 10496/12870: train/loss-mean128 0.0085\n",
      "{'train/loss-last-step128': 0.0, 'lr': 4.339638935340332e-07}\n",
      "step 10624/12870: train/loss-mean128 0.0090\n",
      "{'train/loss-last-step128': 7.450578465295621e-08, 'lr': 3.8965278932759587e-07}\n",
      "step 10752/12870: train/loss-mean128 0.0119\n",
      "{'train/loss-last-step128': 0.0, 'lr': 3.475360207629297e-07}\n",
      "step 10880/12870: train/loss-mean128 0.0253\n",
      "{'train/loss-last-step128': 0.0, 'lr': 3.0765738078837646e-07}\n",
      "step 11008/12870: train/loss-mean128 0.0045\n",
      "{'train/loss-last-step128': 0.0, 'lr': 2.7005833514976127e-07}\n",
      "step 11136/12870: train/loss-mean128 0.0051\n",
      "{'train/loss-last-step128': 1.0430808572436945e-07, 'lr': 2.3477797927437845e-07}\n",
      "step 11264/12870: train/loss-mean128 0.0088\n",
      "{'train/loss-last-step128': 0.00012598543253261596, 'lr': 2.018529976196293e-07}\n",
      "step 11392/12870: train/loss-mean128 0.0370\n",
      "{'train/loss-last-step128': 0.0, 'lr': 1.71317625528582e-07}\n",
      "step 11520/12870: train/loss-mean128 0.0100\n",
      "{'train/loss-last-step128': 0.0, 'lr': 1.4320361363211633e-07}\n",
      "step 11648/12870: train/loss-mean128 0.0079\n",
      "{'train/loss-last-step128': 2.9802318834981634e-08, 'lr': 1.1754019483466883e-07}\n",
      "step 11776/12870: train/loss-mean128 0.0497\n",
      "{'train/loss-last-step128': 0.0, 'lr': 9.435405391790526e-08}\n",
      "step 11904/12870: train/loss-mean128 0.0084\n",
      "{'train/loss-last-step128': 0.0, 'lr': 7.366929979392951e-08}\n",
      "step 12032/12870: train/loss-mean128 0.0154\n",
      "{'train/loss-last-step128': 0.0, 'lr': 5.550744043687245e-08}\n",
      "step 12160/12870: train/loss-mean128 0.0109\n",
      "{'train/loss-last-step128': 0.0, 'lr': 3.988736051893677e-08}\n",
      "step 12288/12870: train/loss-mean128 0.0195\n",
      "{'train/loss-last-step128': 0.0, 'lr': 2.6825301774145994e-08}\n",
      "step 12416/12870: train/loss-mean128 0.0116\n",
      "{'train/loss-last-step128': 0.0, 'lr': 1.6334846110213076e-08}\n",
      "step 12544/12870: train/loss-mean128 0.0076\n",
      "{'train/loss-last-step128': 0.0, 'lr': 8.426901486096316e-09}\n",
      "step 12672/12870: train/loss-mean128 0.0029\n",
      "{'train/loss-last-step128': 0.0, 'lr': 3.1096905699204427e-09}\n",
      "step 12800/12870: train/loss-mean128 0.0015\n",
      "{'train/loss-last-step128': 0.0, 'lr': 3.887421890605425e-10}\n",
      "Calculating embeddings for fact checks\n",
      "Calculating 44221 vectors.\n",
      "Calculating embeddings for posts\n",
      "Calculating 4389 vectors.\n",
      "Calculating similarity for data splits\n",
      "6 ranks produced.\n",
      "step 12870/12870: dev/mean-ps@10 0.1952\n"
     ]
    }
   ],
   "source": [
    "train_dataset = OurDataset(split='train', fold=1).load()\n",
    "dev_dataset = {'dev_eng': OurDataset(split='test', fold=1).load()}\n",
    "\n",
    "cfg = Config()\n",
    "cfg.train.num_epochs = 5\n",
    "cfg.model.name = 'intfloat/multilingual-e5-large-instruct'\n",
    "cfg.model.pooling = 'mean'\n",
    "train(cfg, train_dataset, dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86442303",
   "metadata": {
    "papermill": {
     "duration": 0.01299,
     "end_time": "2024-12-25T02:09:39.403840",
     "exception": false,
     "start_time": "2024-12-25T02:09:39.390850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc93bc",
   "metadata": {
    "papermill": {
     "duration": 0.012954,
     "end_time": "2024-12-25T02:09:39.429761",
     "exception": false,
     "start_time": "2024-12-25T02:09:39.416807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd482f",
   "metadata": {
    "papermill": {
     "duration": 0.012991,
     "end_time": "2024-12-25T02:09:39.455722",
     "exception": false,
     "start_time": "2024-12-25T02:09:39.442731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c49ce",
   "metadata": {
    "papermill": {
     "duration": 0.012796,
     "end_time": "2024-12-25T02:09:39.481383",
     "exception": false,
     "start_time": "2024-12-25T02:09:39.468587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de3816",
   "metadata": {
    "papermill": {
     "duration": 0.01285,
     "end_time": "2024-12-25T02:09:39.507197",
     "exception": false,
     "start_time": "2024-12-25T02:09:39.494347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b25589",
   "metadata": {
    "papermill": {
     "duration": 0.012867,
     "end_time": "2024-12-25T02:09:39.533060",
     "exception": false,
     "start_time": "2024-12-25T02:09:39.520193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bcfbe1",
   "metadata": {
    "papermill": {
     "duration": 0.012887,
     "end_time": "2024-12-25T02:09:39.558967",
     "exception": false,
     "start_time": "2024-12-25T02:09:39.546080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6185840,
     "sourceId": 10282398,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17130.63027,
   "end_time": "2024-12-25T02:09:42.817374",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-24T21:24:12.187104",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "037068f2ccab4d00bbb6bfff9b633c1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f665a0f13415480fac54dc298ec9120c",
       "placeholder": "​",
       "style": "IPY_MODEL_96a5f4ee9ec2450880404f59fa4af20f",
       "tabbable": null,
       "tooltip": null,
       "value": " 964/964 [00:00&lt;00:00, 115kB/s]"
      }
     },
     "0595b33882f94ead9ff6012f739df3b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8973eb0146e04d789f2729ae2c96b4fe",
       "placeholder": "​",
       "style": "IPY_MODEL_f8189a8d3b5d4706a79b0f294e33b7df",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer.json: 100%"
      }
     },
     "07faba77165c43d58c9495cccbcf4cac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1697efc9915a493c88c0892965c49c99": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1d8e231ece744d97817a5d3b34f6187a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0595b33882f94ead9ff6012f739df3b9",
        "IPY_MODEL_b36694edaba74480b4117f83f8bf44de",
        "IPY_MODEL_83475f403bec4c4eafb221db8e00b761"
       ],
       "layout": "IPY_MODEL_9f6c450bb2ad4d709e4ca960968e79e5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "20dd1fee024547958482579e0113b98f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5cede5d6130b4bffaf9c66827a62435c",
       "placeholder": "​",
       "style": "IPY_MODEL_47485f90295947ecaca11f8388a04576",
       "tabbable": null,
       "tooltip": null,
       "value": " 5.07M/5.07M [00:00&lt;00:00, 43.5MB/s]"
      }
     },
     "28472dc09e394585b58dc1d41e5252a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3e77ee592a3d47d38cd4d7e7e34085e8",
       "max": 964,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_85ef6c48c9024411b65d11f61bd2066e",
       "tabbable": null,
       "tooltip": null,
       "value": 964
      }
     },
     "30eb8a53c0d84054b2bf30284d0582ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3428a48e13704b53a702d66356bd2cf4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "36ac1d53839c42a98cdbd1cfc4fffc7b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3e77ee592a3d47d38cd4d7e7e34085e8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "47485f90295947ecaca11f8388a04576": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "499f293b0e03476e91b54c6b35317dee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7e9bfc8527c740b38d77d6a3b8213618",
       "max": 1182,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d364862553494de6a721847a4b71539e",
       "tabbable": null,
       "tooltip": null,
       "value": 1182
      }
     },
     "4c581f1ffcb54896a086091b76d92f10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_71d967198ca84788b594e013d7119735",
       "placeholder": "​",
       "style": "IPY_MODEL_cdfb8e9d4dac453f83205db665dbadd7",
       "tabbable": null,
       "tooltip": null,
       "value": " 690/690 [00:00&lt;00:00, 79.2kB/s]"
      }
     },
     "4ff082dc896a4beba47ed0040cdef564": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6a49793df14546ad8874b39d224cc891",
       "placeholder": "​",
       "style": "IPY_MODEL_9eb8ae7abaf34e6b9ab14e399f9a1cf7",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json: 100%"
      }
     },
     "512fa1e2d1414774a7b113ce551ad3a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "51c47fd05ba748fb812f06e59993a77d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5c8c683eeff94929b8a6be1aadd9ece4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5cede5d6130b4bffaf9c66827a62435c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5de86cf56af247d5a3ba9f3783e462f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6a49793df14546ad8874b39d224cc891": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6e5fcd67cfbb4e6aaabe2575dbcbbdf1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "71d967198ca84788b594e013d7119735": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "73f9ba243b3c4e309d215f9a41c8882e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b5c458d5e4644946950b23b86087fb6b",
        "IPY_MODEL_ee40df203b294f9db1322f9ab9bc1fd9",
        "IPY_MODEL_4c581f1ffcb54896a086091b76d92f10"
       ],
       "layout": "IPY_MODEL_6e5fcd67cfbb4e6aaabe2575dbcbbdf1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "778cd95cf5df42a2ad313f440a9684c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7bb6ce77d10d4fc2b434feb39ea92c68": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7dd995feddae4eeeb50268f438147cba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e9bfc8527c740b38d77d6a3b8213618": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "81ab3d611bc94f8596dd3bd33722c838": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "83475f403bec4c4eafb221db8e00b761": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1697efc9915a493c88c0892965c49c99",
       "placeholder": "​",
       "style": "IPY_MODEL_36ac1d53839c42a98cdbd1cfc4fffc7b",
       "tabbable": null,
       "tooltip": null,
       "value": " 17.1M/17.1M [00:00&lt;00:00, 42.5MB/s]"
      }
     },
     "84d7ac41d5ad486b9564428aef76edb1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4ff082dc896a4beba47ed0040cdef564",
        "IPY_MODEL_28472dc09e394585b58dc1d41e5252a5",
        "IPY_MODEL_037068f2ccab4d00bbb6bfff9b633c1d"
       ],
       "layout": "IPY_MODEL_5de86cf56af247d5a3ba9f3783e462f4",
       "tabbable": null,
       "tooltip": null
      }
     },
     "85ef6c48c9024411b65d11f61bd2066e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8973eb0146e04d789f2729ae2c96b4fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8b26cb0731a04e949dd39aa6e3d19349": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8b3c095b4a0b4daa929951a3800e33d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8e72b1f4a5eb4ba1a061e2c734ce0249": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "921c8f2e109444cc97fe4253f208eb2c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e83e5ab1379e4f439822b75287b60123",
       "placeholder": "​",
       "style": "IPY_MODEL_b59d1eaca0834a65b2e66ae3e9b80425",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "9280df2db81048c8bf3343f5d208c39f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "96a5f4ee9ec2450880404f59fa4af20f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "99406d6d32a943f599cdf9c29c37e4a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cc9c23b30d264a38835c425252c6625e",
        "IPY_MODEL_499f293b0e03476e91b54c6b35317dee",
        "IPY_MODEL_a9472896ad404658b1b13bc92c29d820"
       ],
       "layout": "IPY_MODEL_7bb6ce77d10d4fc2b434feb39ea92c68",
       "tabbable": null,
       "tooltip": null
      }
     },
     "9eb8ae7abaf34e6b9ab14e399f9a1cf7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9f6c450bb2ad4d709e4ca960968e79e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a15e0e03f19e4fd18bfbc357c9d4b9ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a5ccba93fb7644c19605b2825f9df56b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_921c8f2e109444cc97fe4253f208eb2c",
        "IPY_MODEL_f23b29fc3b0a49b283a4efe8708c0420",
        "IPY_MODEL_f463a41f568449938fdc9eeffe57498b"
       ],
       "layout": "IPY_MODEL_9280df2db81048c8bf3343f5d208c39f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a64dc070a09543cd9bbaa713d68c6e78": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a7d890c876f64f00bfbf7c235e43787d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e2a9703323454fbcaa18c140efcf9fb5",
        "IPY_MODEL_e0ec143dd44b43d38a081713b9e693a9",
        "IPY_MODEL_20dd1fee024547958482579e0113b98f"
       ],
       "layout": "IPY_MODEL_778cd95cf5df42a2ad313f440a9684c3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a9472896ad404658b1b13bc92c29d820": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a15e0e03f19e4fd18bfbc357c9d4b9ff",
       "placeholder": "​",
       "style": "IPY_MODEL_8b3c095b4a0b4daa929951a3800e33d0",
       "tabbable": null,
       "tooltip": null,
       "value": " 1.18k/1.18k [00:00&lt;00:00, 124kB/s]"
      }
     },
     "aca7a7e417b84a04a2143f1407b8fbff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ae99b78cbf8f414a99deb54b35d55c31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b36694edaba74480b4117f83f8bf44de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c89bbe6aa41546c4be043bf576cd42f9",
       "max": 17082756,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8e72b1f4a5eb4ba1a061e2c734ce0249",
       "tabbable": null,
       "tooltip": null,
       "value": 17082756
      }
     },
     "b59d1eaca0834a65b2e66ae3e9b80425": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b5c458d5e4644946950b23b86087fb6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_30eb8a53c0d84054b2bf30284d0582ad",
       "placeholder": "​",
       "style": "IPY_MODEL_512fa1e2d1414774a7b113ce551ad3a1",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: 100%"
      }
     },
     "c89bbe6aa41546c4be043bf576cd42f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cc9c23b30d264a38835c425252c6625e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a64dc070a09543cd9bbaa713d68c6e78",
       "placeholder": "​",
       "style": "IPY_MODEL_07faba77165c43d58c9495cccbcf4cac",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json: 100%"
      }
     },
     "cdfb8e9d4dac453f83205db665dbadd7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d0f0faa187574d8dba7e1f18fd2ce514": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d364862553494de6a721847a4b71539e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d56095c6f766432ebcb950c0641eb349": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e0ec143dd44b43d38a081713b9e693a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7dd995feddae4eeeb50268f438147cba",
       "max": 5069051,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_aca7a7e417b84a04a2143f1407b8fbff",
       "tabbable": null,
       "tooltip": null,
       "value": 5069051
      }
     },
     "e2a9703323454fbcaa18c140efcf9fb5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ae99b78cbf8f414a99deb54b35d55c31",
       "placeholder": "​",
       "style": "IPY_MODEL_3428a48e13704b53a702d66356bd2cf4",
       "tabbable": null,
       "tooltip": null,
       "value": "sentencepiece.bpe.model: 100%"
      }
     },
     "e83e5ab1379e4f439822b75287b60123": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ee40df203b294f9db1322f9ab9bc1fd9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8b26cb0731a04e949dd39aa6e3d19349",
       "max": 690,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_81ab3d611bc94f8596dd3bd33722c838",
       "tabbable": null,
       "tooltip": null,
       "value": 690
      }
     },
     "f23b29fc3b0a49b283a4efe8708c0420": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d56095c6f766432ebcb950c0641eb349",
       "max": 1119825680,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d0f0faa187574d8dba7e1f18fd2ce514",
       "tabbable": null,
       "tooltip": null,
       "value": 1119825680
      }
     },
     "f463a41f568449938fdc9eeffe57498b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5c8c683eeff94929b8a6be1aadd9ece4",
       "placeholder": "​",
       "style": "IPY_MODEL_51c47fd05ba748fb812f06e59993a77d",
       "tabbable": null,
       "tooltip": null,
       "value": " 1.12G/1.12G [00:26&lt;00:00, 42.4MB/s]"
      }
     },
     "f665a0f13415480fac54dc298ec9120c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f8189a8d3b5d4706a79b0f294e33b7df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
