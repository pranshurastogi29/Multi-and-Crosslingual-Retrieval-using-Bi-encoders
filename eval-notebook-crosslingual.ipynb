{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10462843,"sourceType":"datasetVersion","datasetId":6467824},{"sourceId":10910871,"sourceType":"datasetVersion","datasetId":6185840}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":7109.775986,"end_time":"2025-01-31T06:05:02.580700","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-01-31T04:06:32.804714","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"011380c528754c4087cf64116fb82636":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"022966db527b4719affb7d925619912d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_299ba9a6d04a405a8c7c337a59f3d6cb","IPY_MODEL_3ddaead31c2f42a2945ba003b48a7f33","IPY_MODEL_9825be5394ce436c94e926beb5211d27"],"layout":"IPY_MODEL_fa3292aa900f4858ad2029b354e9b16a","tabbable":null,"tooltip":null}},"046cb5b67f51447082469400d2911504":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0626e7dbbf3542c0ab39e70037efe01d":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c300f8464c54426b859b9c016ae93a4":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10ced53be484406a81b43782eb09b74e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17a08f37101f4d76af0ba5866e4e8ca5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_0626e7dbbf3542c0ab39e70037efe01d","max":690,"min":0,"orientation":"horizontal","style":"IPY_MODEL_282968809b6c4169bef73eea7f880b84","tabbable":null,"tooltip":null,"value":690}},"1a9476b189994e1192ed285b4d19c15f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4103122c29b445c6bba2480925d61399","IPY_MODEL_50072858b9db42f581d97c95fc42d462","IPY_MODEL_45b1ff54087c4a138938a5c2c9ace850"],"layout":"IPY_MODEL_9f2cbd730b294406a22cefd7a70a7d14","tabbable":null,"tooltip":null}},"2114ed7729594a6686664fc8ad652a1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_594fd135b7414e23b3d689d7d2ab1bb7","placeholder":"​","style":"IPY_MODEL_d4f298a4d818459cb621760177b12a69","tabbable":null,"tooltip":null,"value":" 690/690 [00:00&lt;00:00, 68.7kB/s]"}},"282968809b6c4169bef73eea7f880b84":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"299ba9a6d04a405a8c7c337a59f3d6cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_fc65009b0e624a15b8ec3dcdb93e869b","placeholder":"​","style":"IPY_MODEL_9f99d7ea45b94be2939f39771a1c2ef1","tabbable":null,"tooltip":null,"value":"sentencepiece.bpe.model: 100%"}},"3647a7f03710443db5d7ceba48c3ac35":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3bf1556365d14768b2d980a04413ba09":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"3ddaead31c2f42a2945ba003b48a7f33":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_45cb2032b35e4c808912a1e41398563a","max":5069051,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7eead2e154a549dba90fefbb29c5adc7","tabbable":null,"tooltip":null,"value":5069051}},"3f8ea9bd24fe4ab6a6aefbb0636bbe6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_0c300f8464c54426b859b9c016ae93a4","placeholder":"​","style":"IPY_MODEL_82e4c8c1f8b6427d9276b37b6210325c","tabbable":null,"tooltip":null,"value":" 17.1M/17.1M [00:00&lt;00:00, 42.8MB/s]"}},"4103122c29b445c6bba2480925d61399":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_dfaeb9158e1b46bcbe5778aa380a55fb","placeholder":"​","style":"IPY_MODEL_51a5405b602d48bc802b55ca8b4679c0","tabbable":null,"tooltip":null,"value":"model.safetensors: 100%"}},"45b1ff54087c4a138938a5c2c9ace850":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_e7bb8df6bcde44d984b7ba7a5b7e76d2","placeholder":"​","style":"IPY_MODEL_df1d2defd5944f9ba197b92c38ce687e","tabbable":null,"tooltip":null,"value":" 1.12G/1.12G [00:26&lt;00:00, 43.3MB/s]"}},"45cb2032b35e4c808912a1e41398563a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45ff59e94db940c394ee5d5260b0d537":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46be2f0036b34bb3996e5eb3b91fc3ff":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4975cdf33e6a46bfb1ed26520248bff1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"50072858b9db42f581d97c95fc42d462":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_ff4590bb17c94c9091957f16a9a2c1f0","max":1119825680,"min":0,"orientation":"horizontal","style":"IPY_MODEL_816c2d241bf14c469f2a9ea4c359dd48","tabbable":null,"tooltip":null,"value":1119825680}},"51a5405b602d48bc802b55ca8b4679c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"51fa51e130f44c0ab6296d8ed0ccece8":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"594fd135b7414e23b3d689d7d2ab1bb7":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6732e47d0c81426d8da3ac06231bd1ff":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"684bbe6b9e684e72aeb612bd91a424c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"70fef97b4d5e4271819823fdb7ea9a44":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_f533de5a80d3481db783b7c619f5ba97","placeholder":"​","style":"IPY_MODEL_af4299eb5ee0423898da193aaf809624","tabbable":null,"tooltip":null,"value":" 1.18k/1.18k [00:00&lt;00:00, 124kB/s]"}},"71913ad29fcf4dde9290c59f23686b9b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74bed6708e18419ea063e4965752ec3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_45ff59e94db940c394ee5d5260b0d537","placeholder":"​","style":"IPY_MODEL_e05cc0123bbc454b861b4ca8b533d29f","tabbable":null,"tooltip":null,"value":"tokenizer.json: 100%"}},"7eead2e154a549dba90fefbb29c5adc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"816c2d241bf14c469f2a9ea4c359dd48":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82e4c8c1f8b6427d9276b37b6210325c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"832e6b29a85049a7ac05720c65202750":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8476056da9ab4374bbf3d45c36efb7a6":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9825be5394ce436c94e926beb5211d27":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_a217c4fa85084b2e9691d18ccb440250","placeholder":"​","style":"IPY_MODEL_51fa51e130f44c0ab6296d8ed0ccece8","tabbable":null,"tooltip":null,"value":" 5.07M/5.07M [00:00&lt;00:00, 41.6MB/s]"}},"99a24440147b41cb87aab039f7a84cb4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"9f0c7fc943354d6da60c2facf37f8888":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_f36ff0f808c1432288bab62171c48fe6","placeholder":"​","style":"IPY_MODEL_684bbe6b9e684e72aeb612bd91a424c6","tabbable":null,"tooltip":null,"value":"tokenizer_config.json: 100%"}},"9f2cbd730b294406a22cefd7a70a7d14":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f99d7ea45b94be2939f39771a1c2ef1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a1bb99244cfd478189c5f5ac9bf43348":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_10ced53be484406a81b43782eb09b74e","placeholder":"​","style":"IPY_MODEL_99a24440147b41cb87aab039f7a84cb4","tabbable":null,"tooltip":null,"value":" 964/964 [00:00&lt;00:00, 96.3kB/s]"}},"a217c4fa85084b2e9691d18ccb440250":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af4299eb5ee0423898da193aaf809624":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"b2134e0f0239437094d15fcfc7deb9f6":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b95df7fed9a74a21b3004acf179ee7e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_fdf844a1c67e46a3884b4fd3ad1f22bb","placeholder":"​","style":"IPY_MODEL_3bf1556365d14768b2d980a04413ba09","tabbable":null,"tooltip":null,"value":"special_tokens_map.json: 100%"}},"bf693c41f4c74a7badfb4920d7ed5ab2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"c420c045c3e243a3a9320f0f01f21617":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c86c88d3374848b88537f001b446d2d8","IPY_MODEL_17a08f37101f4d76af0ba5866e4e8ca5","IPY_MODEL_2114ed7729594a6686664fc8ad652a1c"],"layout":"IPY_MODEL_011380c528754c4087cf64116fb82636","tabbable":null,"tooltip":null}},"c86c88d3374848b88537f001b446d2d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_46be2f0036b34bb3996e5eb3b91fc3ff","placeholder":"​","style":"IPY_MODEL_bf693c41f4c74a7badfb4920d7ed5ab2","tabbable":null,"tooltip":null,"value":"config.json: 100%"}},"d4f298a4d818459cb621760177b12a69":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"d8eec57eb969423688f6a16cc7851918":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"df1d2defd5944f9ba197b92c38ce687e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"dfaeb9158e1b46bcbe5778aa380a55fb":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e05cc0123bbc454b861b4ca8b533d29f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"e64b1dd7c4924b22be748c68f2f768f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b95df7fed9a74a21b3004acf179ee7e8","IPY_MODEL_eb7595e8a9e140bc978ca1ca9357a0c5","IPY_MODEL_a1bb99244cfd478189c5f5ac9bf43348"],"layout":"IPY_MODEL_b2134e0f0239437094d15fcfc7deb9f6","tabbable":null,"tooltip":null}},"e6e5c4a701fb460eaba9e7aebb006591":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f0c7fc943354d6da60c2facf37f8888","IPY_MODEL_f016925d0a304b8d91bc37a1fc96243c","IPY_MODEL_70fef97b4d5e4271819823fdb7ea9a44"],"layout":"IPY_MODEL_832e6b29a85049a7ac05720c65202750","tabbable":null,"tooltip":null}},"e7bb8df6bcde44d984b7ba7a5b7e76d2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb7595e8a9e140bc978ca1ca9357a0c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_046cb5b67f51447082469400d2911504","max":964,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3647a7f03710443db5d7ceba48c3ac35","tabbable":null,"tooltip":null,"value":964}},"f016925d0a304b8d91bc37a1fc96243c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_71913ad29fcf4dde9290c59f23686b9b","max":1182,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d8eec57eb969423688f6a16cc7851918","tabbable":null,"tooltip":null,"value":1182}},"f36ff0f808c1432288bab62171c48fe6":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4e68165449442d6af8cc9ad0b079221":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_74bed6708e18419ea063e4965752ec3f","IPY_MODEL_fe8fb309205a412eafd59779ae999f54","IPY_MODEL_3f8ea9bd24fe4ab6a6aefbb0636bbe6d"],"layout":"IPY_MODEL_6732e47d0c81426d8da3ac06231bd1ff","tabbable":null,"tooltip":null}},"f533de5a80d3481db783b7c619f5ba97":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa3292aa900f4858ad2029b354e9b16a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc65009b0e624a15b8ec3dcdb93e869b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdf844a1c67e46a3884b4fd3ad1f22bb":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe8fb309205a412eafd59779ae999f54":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_8476056da9ab4374bbf3d45c36efb7a6","max":17082756,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4975cdf33e6a46bfb1ed26520248bff1","tabbable":null,"tooltip":null,"value":17082756}},"ff4590bb17c94c9091957f16a9a2c1f0":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"09775f18","cell_type":"code","source":"!pip install -q pyterrier\n!pip install -q unidecode","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:18.289020Z","iopub.execute_input":"2025-04-24T03:46:18.289229Z","iopub.status.idle":"2025-04-24T03:46:24.680289Z","shell.execute_reply.started":"2025-04-24T03:46:18.289210Z","shell.execute_reply":"2025-04-24T03:46:24.679098Z"},"papermill":{"duration":8.043041,"end_time":"2025-01-31T04:06:43.398675","exception":false,"start_time":"2025-01-31T04:06:35.355634","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":4},{"id":"42e7174b","cell_type":"code","source":"import torch\nfrom typing import Optional, Tuple\n\ndef broadcast(src: torch.Tensor, other: torch.Tensor, dim: int):\n    if dim < 0:\n        dim = other.dim() + dim\n    if src.dim() == 1:\n        for _ in range(0, dim):\n            src = src.unsqueeze(0)\n    for _ in range(src.dim(), other.dim()):\n        src = src.unsqueeze(-1)\n    src = src.expand(other.size())\n    return src\n\n\ndef scatter_sum(src: torch.Tensor,\n                index: torch.Tensor,\n                dim: int = -1,\n                out: Optional[torch.Tensor] = None,\n                dim_size: Optional[int] = None) -> torch.Tensor:\n    index = broadcast(index, src, dim)\n    if out is None:\n        size = list(src.size())\n        if dim_size is not None:\n            size[dim] = dim_size\n        elif index.numel() == 0:\n            size[dim] = 0\n        else:\n            size[dim] = int(index.max()) + 1\n        out = torch.zeros(size, dtype=src.dtype, device=src.device)\n        return out.scatter_add_(dim, index, src)\n    else:\n        return out.scatter_add_(dim, index, src)\n\n\ndef scatter_add(src: torch.Tensor,\n                index: torch.Tensor,\n                dim: int = -1,\n                out: Optional[torch.Tensor] = None,\n                dim_size: Optional[int] = None) -> torch.Tensor:\n    return scatter_sum(src, index, dim, out, dim_size)\n\n\ndef scatter_mul(src: torch.Tensor,\n                index: torch.Tensor,\n                dim: int = -1,\n                out: Optional[torch.Tensor] = None,\n                dim_size: Optional[int] = None) -> torch.Tensor:\n    return torch.ops.torch_scatter.scatter_mul(src, index, dim, out, dim_size)\n\n\ndef scatter_mean(src: torch.Tensor,\n                 index: torch.Tensor,\n                 dim: int = -1,\n                 out: Optional[torch.Tensor] = None,\n                 dim_size: Optional[int] = None) -> torch.Tensor:\n    out = scatter_sum(src, index, dim, out, dim_size)\n    dim_size = out.size(dim)\n\n    index_dim = dim\n    if index_dim < 0:\n        index_dim = index_dim + src.dim()\n    if index.dim() <= index_dim:\n        index_dim = index.dim() - 1\n\n    ones = torch.ones(index.size(), dtype=src.dtype, device=src.device)\n    count = scatter_sum(ones, index, index_dim, None, dim_size)\n    count[count < 1] = 1\n    count = broadcast(count, out, dim)\n    if out.is_floating_point():\n        out.true_divide_(count)\n    else:\n        out.div_(count, rounding_mode='floor')\n    return out\n\n\ndef scatter_min(\n        src: torch.Tensor,\n        index: torch.Tensor,\n        dim: int = -1,\n        out: Optional[torch.Tensor] = None,\n        dim_size: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n    return torch.ops.torch_scatter.scatter_min(src, index, dim, out, dim_size)\n\n\ndef scatter_max(\n        src: torch.Tensor,\n        index: torch.Tensor,\n        dim: int = -1,\n        out: Optional[torch.Tensor] = None,\n        dim_size: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n    return torch.ops.torch_scatter.scatter_max(src, index, dim, out, dim_size)\n\n\ndef scatter(src: torch.Tensor,\n            index: torch.Tensor,\n            dim: int = -1,\n            out: Optional[torch.Tensor] = None,\n            dim_size: Optional[int] = None,\n            reduce: str = \"sum\") -> torch.Tensor:\n    r\"\"\"\n    |\n\n    .. image:: https://raw.githubusercontent.com/rusty1s/pytorch_scatter/\n            master/docs/source/_figures/add.svg?sanitize=true\n        :align: center\n        :width: 400px\n\n    |\n\n    Reduces all values from the :attr:`src` tensor into :attr:`out` at the\n    indices specified in the :attr:`index` tensor along a given axis\n    :attr:`dim`.\n    For each value in :attr:`src`, its output index is specified by its index\n    in :attr:`src` for dimensions outside of :attr:`dim` and by the\n    corresponding value in :attr:`index` for dimension :attr:`dim`.\n    The applied reduction is defined via the :attr:`reduce` argument.\n\n    Formally, if :attr:`src` and :attr:`index` are :math:`n`-dimensional\n    tensors with size :math:`(x_0, ..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})`\n    and :attr:`dim` = `i`, then :attr:`out` must be an :math:`n`-dimensional\n    tensor with size :math:`(x_0, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})`.\n    Moreover, the values of :attr:`index` must be between :math:`0` and\n    :math:`y - 1`, although no specific ordering of indices is required.\n    The :attr:`index` tensor supports broadcasting in case its dimensions do\n    not match with :attr:`src`.\n\n    For one-dimensional tensors with :obj:`reduce=\"sum\"`, the operation\n    computes\n\n    .. math::\n        \\mathrm{out}_i = \\mathrm{out}_i + \\sum_j~\\mathrm{src}_j\n\n    where :math:`\\sum_j` is over :math:`j` such that\n    :math:`\\mathrm{index}_j = i`.\n\n    .. note::\n\n        This operation is implemented via atomic operations on the GPU and is\n        therefore **non-deterministic** since the order of parallel operations\n        to the same value is undetermined.\n        For floating-point variables, this results in a source of variance in\n        the result.\n\n    :param src: The source tensor.\n    :param index: The indices of elements to scatter.\n    :param dim: The axis along which to index. (default: :obj:`-1`)\n    :param out: The destination tensor.\n    :param dim_size: If :attr:`out` is not given, automatically create output\n        with size :attr:`dim_size` at dimension :attr:`dim`.\n        If :attr:`dim_size` is not given, a minimal sized output tensor\n        according to :obj:`index.max() + 1` is returned.\n    :param reduce: The reduce operation (:obj:`\"sum\"`, :obj:`\"mul\"`,\n        :obj:`\"mean\"`, :obj:`\"min\"` or :obj:`\"max\"`). (default: :obj:`\"sum\"`)\n\n    :rtype: :class:`Tensor`\n\n    .. code-block:: python\n\n        from torch_scatter import scatter\n\n        src = torch.randn(10, 6, 64)\n        index = torch.tensor([0, 1, 0, 1, 2, 1])\n\n        # Broadcasting in the first and last dim.\n        out = scatter(src, index, dim=1, reduce=\"sum\")\n\n        print(out.size())\n\n    .. code-block::\n\n        torch.Size([10, 3, 64])\n    \"\"\"\n    if reduce == 'sum' or reduce == 'add':\n        return scatter_sum(src, index, dim, out, dim_size)\n    if reduce == 'mul':\n        return scatter_mul(src, index, dim, out, dim_size)\n    elif reduce == 'mean':\n        return scatter_mean(src, index, dim, out, dim_size)\n    elif reduce == 'min':\n        return scatter_min(src, index, dim, out, dim_size)[0]\n    elif reduce == 'max':\n        return scatter_max(src, index, dim, out, dim_size)[0]\n    else:\n        raise ValueError\n","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:24.681573Z","iopub.execute_input":"2025-04-24T03:46:24.681849Z","iopub.status.idle":"2025-04-24T03:46:24.698847Z","shell.execute_reply.started":"2025-04-24T03:46:24.681826Z","shell.execute_reply":"2025-04-24T03:46:24.698153Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":3.110382,"end_time":"2025-01-31T04:06:46.522167","exception":false,"start_time":"2025-01-31T04:06:43.411785","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":5},{"id":"211f6354","cell_type":"code","source":"from dataclasses import dataclass, asdict, field\nimport os\nfrom typing import Any, Dict, Optional\nimport yaml\nimport re\nimport string\nimport datetime\nimport ast\nfrom os.path import join as join_path\nfrom collections import *\nimport argparse\nimport copy\nimport glob\nfrom functools import partial\nfrom statistics import mean, median, StatisticsError\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import get_cosine_with_hard_restarts_schedule_with_warmup\nfrom torch import nn\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, T5Config\nimport collections\nfrom torch.optim import Optimizer\nimport itertools\nfrom nltk.tokenize import sent_tokenize\nimport numpy as np\nimport json\nfrom typing import *\nimport math\nimport random\nimport statistics\nfrom scipy.stats import norm\nfrom argparse import Namespace\nimport logging\nimport os\nimport shutil\nimport string\nimport pandas as pd\nimport pyterrier as pt\nfrom unidecode import unidecode\n\n@dataclass\nclass Model:\n    name: str = 'sentence-transformers/all-mpnet-base-v2'\n    pooling: str = 'default'  # 'mean', 'default'\n\n\n@dataclass\nclass Train:\n    device: str = 'cuda' \n    dtype: str = 'float16'  # float32, float16, bfloat16\n    batch_size: int = 16\n    num_epochs: int = 20\n    accumulation_steps: int = 4\n    metrics_window_size: int = 128\n    warmup_steps: int = 400\n    evaluation_steps: int = -1  # If -1, after each epoch\n    label_smoothing: float = 0.  # [0, 1]\n    ema: bool = False  # Exponential moving average\n    freezing: bool = False  # Gradually makes early modules untrainable\n    output_path: Optional[str] = os.path.join('/kaggle/working/')  # If None, no model checkpointing every evaluation_steps\n    save_vectors: bool = False  # Whether to save vectors for evaluation\n    save_models: bool = True # Whether to save models\n    p_max_seq_length: int = 1024\n    fc_max_seq_length: int = 1024\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n@dataclass\nclass Optimizer:\n    name: str = 'adamw' \n    lr: float = 5e-5\n    lr_transformer: float = 5e-5\n    lr_custom: float = 1e-4\n    weight_decay: float = 0.005 \n    clip_value: float = 1.0 \n    sam: bool = False\n    sam_step: float = 1. \n    sam_rho: float = 0.05 \n\n    \n@dataclass\nclass Config:\n    model: Model = Model()\n    train: Train = Train()\n    optimizer: Optimizer = Optimizer()\n\n    seed: int = 3407\n\n    def __init__(self, path: Optional[str] = None):\n        self.timestamp = datetime.datetime.now().strftime('%d-%m-%Y-%H-%M-%S')\n        if path is not None: \n            with open(path, 'r') as f: self.init_class(yaml.load(f, Loader=yaml.SafeLoader))\n        \n    def to_dict(self):\n        return asdict(self)\n    \n    def init_class(self, d):\n        for name in dir(self):\n            if name.startswith('_') or name.endswith('_') or name not in d:\n                continue\n            attr = getattr(self, name)\n            if isinstance(d[name], dict):\n                for k, v in d[name].items():\n                    setattr(attr, k, v)\n            else: \n                setattr(self, name, d[name])","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:24.700406Z","iopub.execute_input":"2025-04-24T03:46:24.700622Z","iopub.status.idle":"2025-04-24T03:46:24.735619Z","shell.execute_reply.started":"2025-04-24T03:46:24.700602Z","shell.execute_reply":"2025-04-24T03:46:24.735017Z"},"papermill":{"duration":7.769398,"end_time":"2025-01-31T04:06:54.304556","exception":false,"start_time":"2025-01-31T04:06:46.535158","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":6},{"id":"f4970e7a","cell_type":"code","source":"\"\"\"\nLibrary of various cleaning-related functions, regular expressions and variables.\n\"\"\"\n\n\nsimple_latin = string.ascii_lowercase + string.ascii_uppercase\ndirty_chars = string.digits + string.punctuation\n\n\ndef is_clean_text(text: str) -> bool:\n    \"\"\"\n    Simple text cleaning method.\n    \"\"\"\n    dirty = (\n        len(text) < 25                                               # Short text\n        or\n        0.5 < sum(char in dirty_chars for char in text) / len(text)  # More than 50% dirty chars                                            \n    )\n    return not dirty\n\n\n# Source: https://gist.github.com/dperini/729294\nurl_regex = re.compile(\n    r'(?:^|(?<![\\w\\/\\.]))'\n    r'(?:(?:https?:\\/\\/|ftp:\\/\\/|www\\d{0,3}\\.))'\n    r'(?:\\S+(?::\\S*)?@)?' r'(?:'\n    r'(?!(?:10|127)(?:\\.\\d{1,3}){3})'\n    r'(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})'\n    r'(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})'\n    r'(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])'\n    r'(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}'\n    r'(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))'\n    r'|'\n    r'(?:(?:[a-z\\\\u00a1-\\\\uffff0-9]-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)'\n    r'(?:\\.(?:[a-z\\\\u00a1-\\\\uffff0-9]-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)*'\n    r'(?:\\.(?:[a-z\\\\u00a1-\\\\uffff]{2,}))' r'|' r'(?:(localhost))' r')'\n    r'(?::\\d{2,5})?'\n    r'(?:\\/[^\\)\\]\\}\\s]*)?',\n    flags=re.IGNORECASE,\n)\n\n\ndef remove_urls(text: str) -> str:\n    return url_regex.sub('', text)\n\n\n# Source: https://gist.github.com/Nikitha2309/15337f4f593c4a21fb0965804755c41d\nemoji_regex = re.compile('['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002500-\\U00002BEF'  # chinese char\n        u'\\U00002702-\\U000027B0'\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        u'\\U0001f926-\\U0001f937'\n        u'\\U00010000-\\U0010ffff'\n        u'\\u2640-\\u2642'\n        u'\\u2600-\\u2B55'\n        u'\\u200d'\n        u'\\u23cf'\n        u'\\u23e9'\n        u'\\u231a'\n        u'\\ufe0f'  # dingbats\n        u'\\u3030'\n    ']+')\n\n\ndef remove_emojis(text: str) -> str:\n    return emoji_regex.sub('', text)\n\n\nsentence_stop_regex = re.compile('['\n    u'\\u002e' # full stop\n    u'\\u2026' # ellipsis\n    u'\\u061F' # arabic question mark\n    u'\\u06D4' # arabic full stop\n    u'\\u2022' # bullet point\n    u'\\u3002' # chinese period\n    u'\\u25CB' # white circle\n    '\\|'      # pipe\n']+')\n\n\ndef replace_stops(text: str) -> str:\n    \"\"\"\n    Replaces some characters that are being used to end sentences. Used for sentence segmentation with sliding windows.\n    \"\"\"\n    return sentence_stop_regex.sub('.', text)\n\n\nwhitespace_regex = re.compile(r'\\s+')\n\n\ndef replace_whitespaces(text: str) -> str:\n    return whitespace_regex.sub(' ', text)\n\n\ndef clean_ocr(ocr: str) -> str:\n    \"\"\"\n    Remove all lines that are shorter than 6 and have more than 50% `dirty_chars`.\n    \"\"\"\n    return '\\n'.join(\n        line\n        for line in ocr.split('\\n')\n        if len(line) > 5 and sum(char in dirty_chars for char in line) / len(line) < 0.5\n    )\n\n\ndef clean_twitter_picture_links(text):\n    \"\"\"\n    Replaces links to picture in twitter post only with 'pic'. \n    \"\"\"\n    return re.sub(r'pic.twitter.com/\\S+', 'pic', text)\n\n\ndef clean_twitter_links(text):\n    \"\"\"\n    Replaces twitter links with 't.co'.\n    \"\"\"\n    return re.sub(r'\\S+//t.co/\\S+', 't.co', text)\n\n\ndef remove_elongation(text):\n    \"\"\"\n    Replaces any occurrence of a string of consecutive identical non-space \n    characters (at least three in a row) with just one instance of that character.\n    \"\"\"\n    text = re.sub(r'(\\S+)\\1{2,}', r'\\1', text)\n    return text\n\ndef safe_literal_eval(value):\n    try:\n        return ast.literal_eval(str(value))\n    except (ValueError, SyntaxError):\n        return value  # Or `None`, depending on how you want to handle it","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:29.323350Z","iopub.execute_input":"2025-04-24T03:46:29.323644Z","iopub.status.idle":"2025-04-24T03:46:29.338407Z","shell.execute_reply.started":"2025-04-24T03:46:29.323622Z","shell.execute_reply":"2025-04-24T03:46:29.337536Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.02833,"end_time":"2025-01-31T04:06:54.345832","exception":false,"start_time":"2025-01-31T04:06:54.317502","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":7},{"id":"22aa3c9e","cell_type":"code","source":"\"\"\"\nLibrary of custom types used for datasets. Some basic functions over these types are also included.\n\"\"\"\n\nId2FactCheck = Dict[int, str]\nId2Post = Dict[int, str]\nFactCheckPostMapping = List[Tuple[int, int]]\n\nLanguage = str  # ISO 639-3 language code\nLanguageDistribution = Dict[Language, float]  # Confidence for language identification. Should sum to [0, 1].\n\nOriginalText = str\nEnglishTranslation = str\nTranslatedText = Tuple[OriginalText, EnglishTranslation, LanguageDistribution]\n\nInstance = Tuple[Optional[datetime.datetime], str]  # When and where was a fact-check or post published\n\n\ndef is_in_distribution(language: Language, distribution: LanguageDistribution, threshold: float = 0.2) -> bool:\n    \"\"\"\n    Check whether `language` is in a `distribution` with more than `treshold` x 100%\n    \"\"\"\n    return next(\n        (\n            percentage >= threshold\n            for distribution_language, percentage in distribution\n            if distribution_language == language\n        ),\n        False\n    )\n\n\ndef combine_distributions(texts: Iterable[TranslatedText]) -> LanguageDistribution:\n    \"\"\"\n    Combine `LanguageDistribution`s from multiple `TranslatedText`s taking the length of the text into consideration.\n    \"\"\"\n    total_length = sum(len(text[0]) for text in texts)\n    distribution = defaultdict(lambda: 0)\n    for original_text, _, text_distribution in texts:\n        for language, percentage in text_distribution:\n            distribution[language] += percentage * len(original_text) / total_length\n    return list(distribution.items())","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:29.720495Z","iopub.execute_input":"2025-04-24T03:46:29.720787Z","iopub.status.idle":"2025-04-24T03:46:29.727168Z","shell.execute_reply.started":"2025-04-24T03:46:29.720763Z","shell.execute_reply":"2025-04-24T03:46:29.726479Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.020298,"end_time":"2025-01-31T04:06:54.378466","exception":false,"start_time":"2025-01-31T04:06:54.358168","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":8},{"id":"9af0800a","cell_type":"code","source":"class Dataset:\n    \"\"\"Dataset\n    \n    Abstract class for datasets. Subclasses should implement `load` function that load `id_to_fact_check`, `id_to_post`,\n    and `fact_check_post_mapping` object attributes. The class also implemenets basic cleaning methods that might be\n    reused.\n    \n    Attributes:\n        clean_ocr: bool = True  Should cleaning of OCRs be performed\n        remove_emojis: bool = False  Should emojis be removed from the texts?\n        remove_urls: bool = True  Should URLs be removed from the texts?\n        replace_whitespaces: bool = True  Should whitespaces be replaced by a single space whitespace?\n        clean_twitter: bool = True  \n        remove_elongation: bool = False  Should occurrence of a string of consecutive identical non-space \n    characters (at least three in a row) with just one instance of that character?\n\n        After `load` is called, following attributes are accesible:\n                fact_check_post_mapping: list[tuple[int, int]]  List of Factcheck-Post id pairs.\n                id_to_fact_check: dict[int, str]  Factcheck id -> Factcheck text\n                id_to_post: dict[int, str]  Post id -> Post text\n                \n    Methods:\n        clean_text: Performs text cleaning based on initialization attributes.\n        maybe_clean_ocr: Perform OCR-specific text cleaning, if `self.clean_ocr`\n        load: Abstract method. To be implemented by the subclasses.\n        \n    \"\"\"\n    \n    # The default values here are based on our preliminary experiments. Might not be the best for all cases.\n    def __init__(\n        self,\n        clean_ocr: bool = True,\n        dataset: str = None,  # Here to read and discard the `dataset` field from the argparser\n        remove_emojis: bool = True,\n        remove_urls: bool = True,\n        replace_whitespaces: bool = True,\n        clean_twitter: bool = True,\n        remove_elongation: bool = False\n    ):\n        self.clean_ocr = clean_ocr\n        self.remove_emojis = remove_emojis\n        self.remove_urls = remove_urls\n        self.replace_whitespaces = replace_whitespaces\n        self.clean_twitter = clean_twitter\n        self.remove_elongation = remove_elongation\n        \n        \n    def __len__(self):\n        return len(self.fact_check_post_mapping)\n\n    \n    def __getitem__(self, idx):\n        p_id, fc_id = self.fact_check_post_mapping[idx]\n        return self.id_to_fact_check[fc_id], self.id_to_post[p_id]\n\n        \n    def clean_text(self, text):\n        \n        if self.remove_urls:\n            text = remove_urls(text)\n\n        if self.remove_emojis:\n            text = remove_emojis(text)\n\n        if self.replace_whitespaces:\n            text = replace_whitespaces(text)\n        \n        if self.clean_twitter:\n            text = clean_twitter_picture_links(text)\n            text = clean_twitter_links(text)\n        \n        if self.remove_elongation:\n            text = remove_elongation(text)\n\n        return text.strip()        \n        \n        \n    def maybe_clean_ocr(self, ocr):\n        if self.clean_ocr:\n            return clean_ocr(ocr)\n        return ocr\n        \n    \n    def __getattr__(self, name):\n        if name in {'id_to_fact_check', 'id_to_post', 'fact_check_post_mapping'}:\n            raise AttributeError(f\"You have to `load` the dataset first before using '{name}'\")\n        raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}'\")\n\n        \n    def load(self):\n        raise NotImplementedError\n        \n","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:30.011788Z","iopub.execute_input":"2025-04-24T03:46:30.012029Z","iopub.status.idle":"2025-04-24T03:46:30.019024Z","shell.execute_reply.started":"2025-04-24T03:46:30.012007Z","shell.execute_reply":"2025-04-24T03:46:30.018236Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.024756,"end_time":"2025-01-31T04:06:54.417210","exception":false,"start_time":"2025-01-31T04:06:54.392454","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":9},{"id":"9efed456","cell_type":"code","source":"\nclass OurDataset(Dataset):\n    \"\"\"Our dataset\n    \n    Class for our dataset that can load different variants. It requires data from `drive/datasets/ours`.\n\n    Initialization Attributes:\n        crosslingual: bool  If `True`, only crosslingual pairs (fact-check and post in different languages) are loaded.\n        fact_check_fields: Iterable[str]  List of fields used to generate the final `str` representation for fact-checks. Supports `claim` and `title`.\n        fact_check_language: Optional[Language]  If a `Language` is specified, only fact-checks with that language are selected.\n        language: Optional[Language]  If a `Language` is specified, only fact-checks and posts with that language are selected.\n        post_language: Optional[Language]  If a `Language` is specified, only posts with that language are selected.\n        split: `train`, `test` or `dev`. `None` means all the samples.\n        version: 'original' or 'english'. Language version of the dataset.\n        \n        Also check `Dataset` attributes.\n        \n        After `load` is called, following attributes are accesible:\n            fact_check_post_mapping: list[tuple[int, int]]  List of Factcheck-Post id pairs.\n            id_to_fact_check: dict[int, str]  Factcheck id -> Factcheck text\n            id_to_post: dict[int, str]  Post id -> Post text\n        \n\n    Methods:\n        load: Loads the data from the csv files. Populates `id_to_fact_check`, `id_to_post` and `fact_check_post_mapping` attributes.\n    \"\"\"\n        \n    our_dataset_path = join_path('/kaggle/input/semeval-data')\n    csvs_loaded = False\n\n    \n    def __init__(\n        self,\n        crosslingual: bool = False,\n        fact_check_fields: Iterable[str] = ('claim', 'title'),\n        fact_check_language: Optional[Language] = None,\n        language: Optional[Language] = None,\n        post_language: Optional[Language] = None,\n        split: Optional[str] = None,\n        # version: str = 'original',\n        fold: int = 0,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        \n        assert all(field in ('claim', 'title') for field in fact_check_fields)\n        assert split in ('test', 'train')\n        \n        # self.crosslingual = crosslingual\n        self.split = split\n        self.fold = fold\n        self.language = language\n        \n    @classmethod\n    def maybe_load_csvs(cls):\n        \"\"\"\n        Load the csvs and store them as class variables. When individual objects are initialized, they can reuse the same\n        pre-loaded dataframes without costly text parsing.\n        \n        `OurDataset.csvs_loaded` is a flag indicating whether the csvs are already loaded.\n        \"\"\"\n        \n        if cls.csvs_loaded:\n            return\n        \n        posts_path = join_path(cls.our_dataset_path, 'test_posts_text.csv')\n        fact_checks_path = join_path(cls.our_dataset_path, 'test_fact_checks_text.csv')\n        \n        for path in [posts_path, fact_checks_path]:\n            assert os.path.isfile(path)\n        \n        print('Loading fact-checks.')\n        cls.df_fact_checks = pd.read_csv(fact_checks_path).fillna('').set_index('fact_check_id')\n        print(f'{len(cls.df_fact_checks)} loaded.')\n\n            \n        print('Loading posts.')\n        cls.df_posts = pd.read_csv(posts_path).fillna('').set_index('post_id')\n        print(f'{len(cls.df_posts)} loaded.')\n\n        cls.csvs_loaded = True\n        \n\n    def load(self):\n        \n        self.maybe_load_csvs()\n        \n        df_posts = self.df_posts.copy()\n        df_fact_checks = self.df_fact_checks.copy()\n\n        print(df_fact_checks['post_lang'].value_counts())\n        \n        if self.language != 'cross':\n\n            df = pd.read_json('/kaggle/input/semeval-data/SemEval_Task7_Test_Phase/tasks.json').reset_index()\n            lang_fact_checks = df[df['index']==self.language]['monolingual'].values[0]['fact_checks']\n            print('task lang fact checks', len(lang_fact_checks))\n            lang_post_dev = df[df['index']==self.language]['monolingual'].values[0]['posts_test']\n            print('task lang post dev', len(lang_post_dev))\n\n            df_fact_checks = df_fact_checks[df_fact_checks.index.isin(lang_fact_checks)]\n            df_posts = df_posts[df_posts.index.isin(lang_post_dev)]\n    \n            print(f'Filtering by split: {len(df_posts)} posts remaining and sampled {len(df_fact_checks)} fact checks')\n            \n        else:\n            \n            df = pd.read_json('/kaggle/input/semeval-data/SemEval_Task7_Test_Phase/tasks.json').reset_index()\n            lang_fact_checks = df[df['index']=='fact_checks']['crosslingual'].dropna().values[0]\n            print('task lang fact checks', len(lang_fact_checks))\n            lang_post_dev = df[df['index']=='posts_test']['crosslingual'].dropna().values[0]\n            print('task lang post dev', len(lang_post_dev))\n\n            df_fact_checks = df_fact_checks[df_fact_checks.index.isin(lang_fact_checks)]\n            df_posts = df_posts[df_posts.index.isin(lang_post_dev)]\n    \n            print(f'Filtering by split: {len(df_posts)} posts remaining and sampled {len(df_fact_checks)} fact checks')\n\n        print(f'Filtering fact-checks by language: {len(df_fact_checks)} posts remaining.')\n        print(f'Filtering posts by language: {len(df_posts)} posts remaining.')\n        print(f'Filtering posts.')\n        print(f'Posts remaining: {len(df_posts)}')\n            \n\n        self.id_to_post = dict()\n        for post_id, post_text in zip(df_posts.index, df_posts['clean_text']):\n            self.id_to_post[post_id] = self.clean_text(post_text)\n\n        self.id_to_fact_check = dict()\n        for fact_check_id, fact_text in zip(df_fact_checks.index, df_fact_checks['clean_text']):\n            self.id_to_fact_check[fact_check_id] = self.clean_text(fact_text)\n            \n        return self\n    ","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:31.392110Z","iopub.execute_input":"2025-04-24T03:46:31.392391Z","iopub.status.idle":"2025-04-24T03:46:31.403577Z","shell.execute_reply.started":"2025-04-24T03:46:31.392371Z","shell.execute_reply":"2025-04-24T03:46:31.402843Z"},"papermill":{"duration":0.027396,"end_time":"2025-01-31T04:06:54.459285","exception":false,"start_time":"2025-01-31T04:06:54.431889","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":10},{"id":"b4033b07","cell_type":"code","source":"df = pd.read_json('/kaggle/input/semeval-data/SemEval_Task7_Test_Phase/tasks.json').reset_index()\nlang_fact_checks = df[df['index']=='fact_checks']['crosslingual'].dropna().values[0]\nprint('task lang fact checks', len(lang_fact_checks))\nlang_post_dev = df[df['index']=='posts_test']['crosslingual'].dropna().values[0]\nprint('task lang post dev', len(lang_post_dev))","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:36.747179Z","iopub.execute_input":"2025-04-24T03:46:36.747626Z","iopub.status.idle":"2025-04-24T03:46:36.880039Z","shell.execute_reply.started":"2025-04-24T03:46:36.747589Z","shell.execute_reply":"2025-04-24T03:46:36.879225Z"},"papermill":{"duration":0.115189,"end_time":"2025-01-31T04:06:54.589613","exception":false,"start_time":"2025-01-31T04:06:54.474424","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"task lang fact checks 272447\ntask lang post dev 4000\n","output_type":"stream"}],"execution_count":11},{"id":"15ca349a","cell_type":"code","source":"def process_result_generator(gen: Generator, default_rank: int = None, csv_path: str = None):\n    \"\"\"\n    Take the results generated from `gen` and process them. By default, only calculate metrics, but dumping the results into a csv file is also supported\n    via `csv_path` attribute. For `default_rank` see `predicted_ranks` function.\n    \"\"\"\n    ranks = list()\n    rows = list()\n    for predicted_ids, probab_ids, post_id in gen:\n        if csv_path:\n            rows.append((post_id, probab_ids[:50], predicted_ids[:50]))\n    if csv_path:\n        pd.DataFrame(rows, columns=['post_id', 'predicted_fact_check_probs',\n                                    'predicted_fact_check_ids']).to_csv(csv_path, index=False)\n    print('done')\ndef result_generator(func):\n    \"\"\"\n    This is a decorator function that should be used on result generators. The generators return by default: `predicted_fact_check_ids` and `post_id`.\n    Here, the results are enriched with the `desired_fact_check_ids` as indicated by `dataset.fact_check_post_mapping`\n    \"\"\"\n    def wrapper(dataset, *args, **kwargs):\n        for predicted_fact_check_ids, probab_ids, post_id in func(dataset, *args, **kwargs):\n            yield predicted_fact_check_ids, probab_ids, post_id\n    return wrapper","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:38.446532Z","iopub.execute_input":"2025-04-24T03:46:38.446824Z","iopub.status.idle":"2025-04-24T03:46:38.457583Z","shell.execute_reply.started":"2025-04-24T03:46:38.446800Z","shell.execute_reply":"2025-04-24T03:46:38.456663Z"},"papermill":{"duration":0.023287,"end_time":"2025-01-31T04:06:54.626630","exception":false,"start_time":"2025-01-31T04:06:54.603343","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":12},{"id":"85bcad17","cell_type":"code","source":"class Vectorizer:\n    \"\"\"\n    An abstract class for vectorizers. Vectorizers calculate vectors for texts and handle thir caching so that we do not have to calculate\n    the vector for the same text more than once.\n    \n    Currently supported:\n            `SentenceTransformerVectorizer` - Used for models using `sentence_transformers` library.\n            `LaserVectorizer` - TBA\n            `PytorchVectorizer` - Used for pytorch models (nn.Module).\n            \n    The main call for `Vectorizer` is `vectorize`. This will calculate the appropriate vectors and store them in the `dict` attribute. The class also\n    supports `save` and `load`. `dir_path` is used as path to a folder where there is a `vocab.json` stored with the collection of texts and `vectors.py`\n    with a torch tensor of vectors for the texts.\n    \"\"\"\n    \n    def __init__(self, dir_path: str):\n        self.dict = {}\n        \n        self.dir_path = dir_path\n        if dir_path:\n            self.vectors_path = os.path.join(dir_path, 'vectors.pt')\n            self.vocab_path = os.path.join(dir_path, 'vocab.json')\n        \n            try:\n                self.load()\n                print(f'Vector database with {len(self.dict)} records loaded')\n            except FileNotFoundError:\n                pass\n                print(f'No previous database found')\n\n        \n    def vectorize(self, texts: List[str], save_if_missing: bool = False, normalize: bool = False) -> torch.tensor:\n        \"\"\"\n        The main API point for the users. Try to find the vectors in the existing database. For the missing texts, the vectors will be calculated\n        and saved in the `self.dict`. \n        \n        Attributes:\n            save_if_missing: bool  Should the vectors in `dict` be saved after new vectors are calculated? This makes sense for models that will\n                be used more than once.\n            normalize: bool  Should the vectors be normalized. Useful for cosine similarity calculations.\n        \"\"\"\n        \n        missing_texts = list(set(texts) - set(self.dict.keys()))\n        \n        if missing_texts:\n            \n            print(f'Calculating {len(missing_texts)} vectors.')\n            missing_vectors = self._calculate_vectors(missing_texts)\n            for text, vector in zip(missing_texts, missing_vectors):\n                self.dict[text] = vector\n            \n            if save_if_missing:\n                self.save()\n            \n        vectors = torch.vstack([\n            self.dict[text]\n            for text in texts\n        ])\n        \n        if normalize:\n            vectors = torch.nn.functional.normalize(vectors, p=2, dim=1)\n            \n        return vectors\n\n    \n    def _calculate_vectors(self, txts: List[str]) -> torch.tensor:\n        \"\"\"\n        Abstract method to be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError\n\n            \n    def load(self):\n        \"\"\"\n        Load vocab and vectors from appropriate files\n        \"\"\"\n        with open(self.vocab_path, 'r', encoding='utf8') as f:\n            vocab = json.load(f)\n        \n        vectors = torch.load(self.vectors_path)\n        \n        assert len(vocab) == len(vectors)\n        \n        self.dict = {\n            text: vector\n            for text, vector in zip(vocab, vectors)\n        }\n        \n        \n    def save(self):\n        \"\"\"\n        Save vocab and vectors to appropriate files\n        \"\"\"\n        os.makedirs(self.dir_path, exist_ok=True)\n            \n        vocab = list(self.dict.keys())        \n        with open(self.vocab_path, 'w', encoding='utf8') as f:\n            json.dump(vocab, f)\n            \n        vectors = torch.vstack(list(self.dict.values()))\n        torch.save(vectors, self.vectors_path)\n            ","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:45.524513Z","iopub.execute_input":"2025-04-24T03:46:45.524842Z","iopub.status.idle":"2025-04-24T03:46:45.533953Z","shell.execute_reply.started":"2025-04-24T03:46:45.524814Z","shell.execute_reply":"2025-04-24T03:46:45.532921Z"},"papermill":{"duration":0.02274,"end_time":"2025-01-31T04:06:54.661567","exception":false,"start_time":"2025-01-31T04:06:54.638827","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":13},{"id":"0cd2d0cf","cell_type":"code","source":"def slice_text(text, window_type, window_size, window_stride=None) -> List[str]:\n    \"\"\"\n    Split a `text` into parts using a sliding window. The windows slides either across characters or sentences, based on the value of `window_tyoe`.\n\n    Attributes:\n        text: str  Text that is to be splitted into windows.\n        window_type: str  Either `sentence` or `character`. The basic unit of the windows.\n        window_size: int  How many units are in a window.\n        window_stride: int  How many units are skipped each time the window moves.\n    \"\"\"\n\n    text = replace_whitespaces(text)\n\n    if window_stride is None:\n        window_stride = window_size\n\n    if window_size < window_stride:\n        print(f'Window size ({window_size}) is smaller than stride length ({window_stride}). This will result in missing chunks of text.')\n\n\n    if window_type == 'sentence':\n        text = replace_stops(text)\n        sentences = sent_tokenize(text)\n        return [\n            ' '.join(sentences[i:i+window_size])\n            for i in range(0, len(sentences), window_stride)\n        ]\n\n    elif window_type == 'character':\n        return [\n            text[i:i+window_size]\n            for i in range(0, len(text), window_stride)\n        ]\n\n\ndef gen_sliding_window_delimiters(post_lengths: List[int], max_size: int) -> Generator[Tuple[int, int], None, None]:\n    \"\"\"\n    Calculate where to split the sequence of `post_lenghts` so that the individual batches do not exceed `max_size`\n    \"\"\"\n    range_length = start = cur_sum = 0\n\n    for post_length in post_lengths:\n        if (range_length + post_length) > max_size: # exceeds memory\n            yield (start, start + range_length)\n            start = cur_sum\n            range_length = post_length\n        else: # memory still avail in current split\n            range_length += post_length\n        cur_sum += post_length\n\n    if range_length > 0:\n        yield (start, start + range_length)\n\n\n@result_generator\ndef embedding_results(\n    dataset: Dataset,\n    vectorizer_fact_check: Vectorizer,\n    vectorizer_post: Vectorizer,\n    sliding_window: bool = False,\n    sliding_window_pooling: str = 'max',\n    sliding_window_size: int = None,\n    sliding_window_stride: int = None,\n    sliding_window_type: str = None,\n    post_split_size: int = 256,\n    dtype: torch.dtype = torch.float16,\n    device: str = 'cpu',\n    save_if_missing: bool = False\n\n):\n    \"\"\"\n    Generate results using cosine similarity based on embeddings generated via vectorizers.\n\n    Attributes:\n        dataset: Dataset\n        vectorizer_fact_check: Vectorizer  Vectorizer used to process fact-checks\n        vectorizer_post: Vectorizer  Vectorizer used to process posts\n        sliding_window: bool  Should sliding window be used or should texts be process without slicing.\n        sliding_window_pooling: str  One of 'sum', 'mul', 'mean', 'min', 'max' as defined here: https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html\n        sliding_window_size, sliding_window_stride, sliding_window_type:  See `slice_text`\n        post_split_size: int  Batch size for post embeddings for sim calculation\n        dtype: torch.dtype  Data type in which calculate sim\n        device: str  Device on which calculate sim\n        save_if_missing: bool  Should the vectors in `dict` be saved after new vectors are calculated? This makes sense for models that will\n                be used more than once.\n    \"\"\"\n\n    print('Calculating embeddings for fact checks')\n    fact_check_embeddings = vectorizer_fact_check.vectorize(\n        dataset.id_to_fact_check.values(),\n        save_if_missing=save_if_missing,\n        normalize=True\n    )\n    fact_check_embeddings = fact_check_embeddings.transpose(0, 1)  # Rotate for matmul\n\n    fact_check_embeddings = fact_check_embeddings.to(device=device, dtype=dtype)\n\n\n    # We need to split the calculations because of memory limitations, sims matrix alone requires 200k x 25k x 4 = ~20GB RAM\n    # memory = 2**30 # assume 4gb free memory - 2**31 = 2gb for both `sims` and `sorted_ids`\n    # post_split_size = memory // len(dataset.id_to_fact_check) // 4  # // 4 because of float32\n    post_ids = iter(dataset.id_to_post.keys())\n\n    if sliding_window:\n\n        print('Splitting posts into windows.')\n        windows = [\n            slice_text(post, sliding_window_type, sliding_window_size, sliding_window_stride)\n            for post in dataset.id_to_post.values()\n        ]\n\n        print('Calculating embeddings for the windows')\n        post_embeddings = vectorizer_post.vectorize(\n            list(itertools.chain(*windows)),\n            save_if_missing=save_if_missing,\n            normalize=True\n        )\n\n        # We need to split the matrix matmul so that all the windows from each post belong to the same batch.\n        post_lengths = [len(post) for post in windows]\n        segment_array = torch.tensor([\n            i\n            for i, num_windows in enumerate(post_lengths)\n            for _ in range(num_windows)\n        ])\n        delimiters = list(gen_sliding_window_delimiters(post_lengths, post_split_size))\n\n        print('Calculating similarity for data splits')\n\n        for start_id, end_id in delimiters:\n\n            sims = torch.mm(\n                post_embeddings[start_id:end_id].to(device=device, dtype=dtype),\n                fact_check_embeddings\n            )\n\n            segments = segment_array[start_id:end_id]\n            segments -= int(segments[0])\n\n            sims = scatter(\n                src=sims,\n                index=segments,\n                dim=0,\n                reduce=sliding_window_pooling,\n            )\n\n            sorted_ids = torch.argsort(sims, descending=True, dim=1)\n\n            fact_check_ids = {i: fc_id for i, fc_id in enumerate(dataset.id_to_fact_check.keys())}\n            for row in sorted_ids:\n                row = row.cpu().numpy()\n                row = np.vectorize(fact_check_ids.__getitem__)(row)\n                yield row, next(post_ids)\n\n\n    else:\n\n        print('Calculating embeddings for posts')\n        post_embeddings = vectorizer_post.vectorize(\n            dataset.id_to_post.values(),\n            save_if_missing=save_if_missing,\n            normalize=True\n        )\n\n        print('Calculating similarity for data splits')\n        for start_id in range(0, len(dataset.id_to_post), post_split_size):\n            end_id = start_id + post_split_size\n\n            sims = torch.mm(\n                post_embeddings[start_id:end_id].to(device=device, dtype=dtype),\n                fact_check_embeddings\n            )\n\n            # TODO: argsort does not duplicities into account, the results might not be deterministic\n            # sorted_ids = torch.argsort(sims, descending=True, dim=1)\n            sorted_sims, sorted_ids = torch.sort(sims, descending=True, dim=1)\n\n            fact_check_ids = {i: fc_id for i, fc_id in enumerate(dataset.id_to_fact_check.keys())}\n            # for row in sorted_ids:\n            #     row = row.cpu().numpy()\n            #     row = np.vectorize(fact_check_ids.__getitem__)(row)\n            #     yield row, next(post_ids)\n\n            for ids_row, sims_row in zip(sorted_ids, sorted_sims):\n                ids_row = ids_row.cpu().numpy()\n                sims_row = sims_row.cpu().numpy()\n\n                fact_ids = np.vectorize(fact_check_ids.__getitem__)(ids_row)\n                yield fact_ids, sims_row, next(post_ids)","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:45.833220Z","iopub.execute_input":"2025-04-24T03:46:45.833485Z","iopub.status.idle":"2025-04-24T03:46:45.846839Z","shell.execute_reply.started":"2025-04-24T03:46:45.833464Z","shell.execute_reply":"2025-04-24T03:46:45.845982Z"},"papermill":{"duration":0.026655,"end_time":"2025-01-31T04:06:54.700699","exception":false,"start_time":"2025-01-31T04:06:54.674044","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":14},{"id":"83675772","cell_type":"code","source":"class PytorchVectorizer(Vectorizer):\n    \"\"\"\n    Vectorizer for `Pytorch` models.\n    \"\"\"\n    \n    def __init__(\n        self,\n        dir_path: str,\n        model_handle: str = None,\n        model: torch.nn.Module = None,\n        tokenizer = None,\n        batch_size: int = 32,\n        dtype: torch.dtype = torch.float16,\n        port_embeddings_to_cpu: bool = True\n    ):\n        \"\"\"\n        Attributes:\n            dir_path: str  Path to cached vectors and vocab files.\n            model_handle: str  Name of the model, either a HuggingFace repository handle or path to a local model.\n            model: SentenceTransformer  A loaded model -- this option can be used during fine-tuning.\n            tokenizer: AutoTokenizer  A tokenizer for the model.\n            batch_size: int  Batch size for inference\n            dtype: torch.dtype  Inference dtype\n            port_embeddings_to_cpu: bool  Whether to move the embeddings to CPU after inference.\n        \"\"\"\n        \n        super().__init__(dir_path)\n        \n        if model_handle:\n            self.model = torch.load(model_handle)\n            self.model.eval()        \n        else:\n            self.model = model\n\n        assert self.model\n\n        self.device = next(self.model.parameters()).device.type\n        self.tokenizer = tokenizer\n        self.batch_size = batch_size\n        self.dtype = dtype\n        self.port_embeddings_to_cpu = port_embeddings_to_cpu\n\n        \n    def _calculate_vectors(self, texts: List[str]) -> torch.tensor:\n\n        @torch.autocast(device_type=self.device.split(':')[0], dtype=self.dtype)\n        @torch.no_grad()\n        def embedding_pipeline(text: List[str], tokenizer, model, device, max_length = 512):\n            tokenized = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors='pt').to(device)\n            embeddings = model(**tokenized)\n            return embeddings.cpu() if self.port_embeddings_to_cpu else embeddings\n\n        return torch.vstack(\n                [\n                    embedding_pipeline(\n                        texts[i:i+self.batch_size], \n                        self.tokenizer, \n                        self.model, \n                        device=self.device, \n                    ) \n                    for i in range(0, len(texts), self.batch_size)\n                ]\n            )\n            ","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:46.110863Z","iopub.execute_input":"2025-04-24T03:46:46.111106Z","iopub.status.idle":"2025-04-24T03:46:46.118053Z","shell.execute_reply.started":"2025-04-24T03:46:46.111086Z","shell.execute_reply":"2025-04-24T03:46:46.117398Z"},"papermill":{"duration":0.019843,"end_time":"2025-01-31T04:06:54.732670","exception":false,"start_time":"2025-01-31T04:06:54.712827","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":15},{"id":"60989515","cell_type":"code","source":"\"\"\"\nhttps://github.com/mosaicml/composer/blob/dev/composer/algorithms/ema/ema.py\nExponential Moving Average (EMA) is a model averaging technique that maintains an \nexponentially weighted moving average of the model parameters during training. \nThe averaged parameters are used for model evaluation. EMA typically results \nin less noisy validation metrics over the course of training, and sometimes \nincreased generalization.\n\"\"\"\n\ndef compute_ema(model: torch.nn.Module, ema_model: torch.nn.Module, smoothing: float = 0.99):\n    with torch.no_grad():\n        model_params = itertools.chain(model.parameters(), model.buffers())\n        ema_model_params = itertools.chain(ema_model.parameters(), ema_model.buffers())\n\n        for ema_param, model_param in zip(ema_model_params, model_params):\n            model_param = model_param.detach()\n            ema_param.copy_(ema_param * smoothing + (1. - smoothing) * model_param)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:46.406940Z","iopub.execute_input":"2025-04-24T03:46:46.407229Z","iopub.status.idle":"2025-04-24T03:46:46.411815Z","shell.execute_reply.started":"2025-04-24T03:46:46.407207Z","shell.execute_reply":"2025-04-24T03:46:46.410941Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.017412,"end_time":"2025-01-31T04:06:54.762224","exception":false,"start_time":"2025-01-31T04:06:54.744812","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":16},{"id":"1cd279b6","cell_type":"code","source":"\"\"\"\nhttps://github.com/mosaicml/composer/blob/dev/composer/algorithms/sam/sam.py\nSharpness-Aware Minimization (SAM) is an optimization algorithm that minimizes \nboth the loss and the sharpness of the loss. It finds parameters that lie in \na neighborhood of low loss. The authors find that this improves model generalization\n\"\"\"\n\nclass SAM(torch.optim.Optimizer):\n    \"\"\"Wraps an optimizer with sharpness-aware minimization (`Foret et al, 2020 <https://arxiv.org/abs/2010.01412>`_).\n    See :class:`.SAM` for details.\n    Implementation based on https://github.com/davda54/sam\n    Args:\n        base_optimizer (torch.optim.Optimizer) The optimizer to apply SAM to.\n        rho (float, optional): The SAM neighborhood size. Must be greater than 0. Default: ``0.05``.\n        epsilon (float, optional): A small value added to the gradient norm for numerical stability. Default: ``1.0e-12``.\n        interval (int, optional): SAM will run once per ``interval`` steps. A value of 1 will\n            cause SAM to run every step. Steps on which SAM runs take\n            roughly twice as much time to complete. Default: ``1``.\n    \"\"\"\n\n    def __init__(\n        self,\n        base_optimizer: torch.optim.Optimizer,\n        rho: float = 0.05,\n        epsilon: float = 1.0e-12,\n        **kwargs\n    ):\n        if rho < 0:\n            raise ValueError(f'Invalid rho, should be non-negative: {rho}')\n        self.base_optimizer = base_optimizer\n        defaults = {'rho': rho, 'epsilon': epsilon, **kwargs}\n        super(SAM, self).__init__(self.base_optimizer.param_groups, defaults)\n\n    @torch.no_grad()\n    def first_step(self):\n        grad_norm = self._grad_norm()\n        for group in self.param_groups:\n            scale = group['rho'] / (grad_norm + group['epsilon'])\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                e_w = p.grad * scale.to(p)\n                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n                self.state[p]['e_w'] = e_w\n\n    @torch.no_grad()\n    def second_step(self):\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None or 'e_w' not in self.state[p]:\n                    continue\n                p.sub_(self.state[p]['e_w'])  # get back to \"w\" from \"w + e(w)\"\n        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n\n    @torch.no_grad()\n    def step(self, step_type: str):\n        \"\"\" Adjusted to PyTorch mixed precision training framework\n        \"\"\"\n        if step_type == 'first':\n            self.first_step()\n        elif step_type == 'second':\n            self.second_step()\n        elif step_type == 'skip':\n            self.base_optimizer.step()\n\n\n    def _grad_norm(self):\n        norm = torch.norm(torch.stack(\n            [p.grad.norm(p=2) for group in self.param_groups for p in group['params'] if p.grad is not None]),\n                          p='fro')\n        return norm","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:46.682223Z","iopub.execute_input":"2025-04-24T03:46:46.682475Z","iopub.status.idle":"2025-04-24T03:46:46.690611Z","shell.execute_reply.started":"2025-04-24T03:46:46.682454Z","shell.execute_reply":"2025-04-24T03:46:46.689942Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.02137,"end_time":"2025-01-31T04:06:54.795937","exception":false,"start_time":"2025-01-31T04:06:54.774567","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":17},{"id":"315b1d31","cell_type":"code","source":"class MNRloss(nn.Module):\n    def __init__(self, label_smoothing=0):\n        super().__init__()\n        self.loss_f = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n\n    def forward(self, sentence_embedding_A: torch.Tensor, sentence_embedding_B: torch.Tensor):\n        # Compute similarity matrix\n        scores = torch.mm(sentence_embedding_A, sentence_embedding_B.transpose(0, 1))\n        # Compute labels\n        labels = torch.arange(len(scores), dtype=torch.long, device=scores.device)\n        return self.loss_f(scores, labels)","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:46.986467Z","iopub.execute_input":"2025-04-24T03:46:46.986687Z","iopub.status.idle":"2025-04-24T03:46:46.991305Z","shell.execute_reply.started":"2025-04-24T03:46:46.986670Z","shell.execute_reply":"2025-04-24T03:46:46.990486Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.018314,"end_time":"2025-01-31T04:06:54.826588","exception":false,"start_time":"2025-01-31T04:06:54.808274","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":18},{"id":"e531af08","cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import AutoModel, AutoTokenizer\n\nclass Model(nn.Module):\n    def __init__(self, model_name, pooling='default', hidden_size=768, lstm_hidden_size=128):\n        super(Model, self).__init__()\n        self.pooling = pooling\n        \n        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n        \n        # self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=lstm_hidden_size, batch_first=True, bidirectional=True)\n        # self.attention = nn.Linear(lstm_hidden_size * 2, 1)  # Bidirectional LSTM\n\n    def forward(self, input_ids, attention_mask, token_type_ids=None):\n        model_output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_states = model_output[0]  # Shape: (batch_size, seq_len, hidden_size)\n\n        if self.pooling == 'default':\n            return model_output[1]\n        elif self.pooling == 'mean':\n            return self.mean_pooling(hidden_states, attention_mask)\n        elif self.pooling == 'attention':\n            return self.attention_pooling(hidden_states, attention_mask)\n    \n    @staticmethod\n    def mean_pooling(token_embeddings: torch.Tensor, attention_mask: torch.Tensor):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    \n    def attention_pooling(self, token_embeddings: torch.Tensor, attention_mask: torch.Tensor):\n        # Pass through LSTM\n        lstm_output, _ = self.lstm(token_embeddings)\n        \n        # Compute attention scores\n        attention_scores = self.attention(lstm_output).squeeze(-1)\n        attention_scores = attention_scores.masked_fill(attention_mask == 0, float('-inf'))\n        attention_weights = torch.softmax(attention_scores, dim=-1)\n        \n        # Compute weighted sum of token embeddings\n        attention_weights = attention_weights.unsqueeze(-1)\n        weighted_sum = torch.sum(lstm_output * attention_weights, dim=1)\n        \n        return weighted_sum\n\ndef get_model_tokenizer(model_name: str, **kwargs):\n    return Model(model_name, **kwargs), AutoTokenizer.from_pretrained(model_name, use_fast=False)","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:47.267675Z","iopub.execute_input":"2025-04-24T03:46:47.267944Z","iopub.status.idle":"2025-04-24T03:46:47.275923Z","shell.execute_reply.started":"2025-04-24T03:46:47.267921Z","shell.execute_reply":"2025-04-24T03:46:47.275260Z"},"papermill":{"duration":0.020829,"end_time":"2025-01-31T04:06:54.859804","exception":false,"start_time":"2025-01-31T04:06:54.838975","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":19},{"id":"c428127d","cell_type":"code","source":"BATCH = Tuple[Dict[str, torch.Tensor]]\n\n\ndef safe_mean(x, round_to=4):\n    try:\n        return round(mean(x), round_to)\n    except StatisticsError:\n        return None\n\ndef evaluate_datasets(datasets, model, tokenizer, run_path, save_vectors, batch_size):\n    model.eval()  # Set model to evaluation mode\n    results = {}\n\n    vectorizer_path = run_path if save_vectors else None\n    vct = PytorchVectorizer(dir_path=vectorizer_path, model=model, tokenizer=tokenizer, \n                            batch_size=batch_size, port_embeddings_to_cpu=True)\n\n    with torch.no_grad():  # Disable gradient computation\n        for dataset_name, dataset in datasets.items():\n            result_gen = embedding_results(dataset, vct, vct, post_split_size=512, device='cuda')\n            process_result_generator(result_gen, csv_path=os.path.join(run_path, dataset_name + '.csv'))\n            \n    return True\n    \n\ndef predict_dataset(cfg, dev_datasets):\n    \n    try:\n        if cfg.train.output_path is not None:\n            run_path = os.path.join(cfg.train.output_path, cfg.timestamp)\n            os.makedirs(run_path)\n            # save config\n            with open(os.path.join(run_path, 'config.yaml') ,'w') as f:\n                yaml.dump(cfg.to_dict(), f, default_flow_style=False)\n    except:\n        pass\n\n    try:\n        model_path = os.path.join(run_path, cfg.model.name)\n        os.makedirs(model_path)\n    except:\n        pass\n\n    print('Get model')\n    model, tokenizer = get_model_tokenizer(cfg.model.name, pooling=cfg.model.pooling)\n\n    # if torch.cuda.device_count() > 1:\n    #     print(f\"Using {torch.cuda.device_count()} GPUs\")\n    #     model = torch.nn.DataParallel(model)\n\n    print('Loading Model Weights')\n    model.load_state_dict(torch.load(cfg.model.path, weights_only=True))\n\n    model = model.to(cfg.train.device)\n\n    print('eval done' , evaluate_datasets({**dev_datasets}, model, tokenizer, \n                                model_path, save_vectors=False, batch_size=cfg.train.batch_size))\n","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:47.521827Z","iopub.execute_input":"2025-04-24T03:46:47.522122Z","iopub.status.idle":"2025-04-24T03:46:47.529798Z","shell.execute_reply.started":"2025-04-24T03:46:47.522098Z","shell.execute_reply":"2025-04-24T03:46:47.529065Z"},"papermill":{"duration":0.020851,"end_time":"2025-01-31T04:06:54.892962","exception":false,"start_time":"2025-01-31T04:06:54.872111","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":20},{"id":"0e8ec929","cell_type":"code","source":"'fra', 'spa', 'eng', 'por', 'tha', 'deu', 'msa', 'ara'","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:47.824157Z","iopub.execute_input":"2025-04-24T03:46:47.824419Z","iopub.status.idle":"2025-04-24T03:46:47.830577Z","shell.execute_reply.started":"2025-04-24T03:46:47.824388Z","shell.execute_reply":"2025-04-24T03:46:47.829760Z"},"papermill":{"duration":0.023632,"end_time":"2025-01-31T04:06:54.928856","exception":false,"start_time":"2025-01-31T04:06:54.905224","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"('fra', 'spa', 'eng', 'por', 'tha', 'deu', 'msa', 'ara')"},"metadata":{}}],"execution_count":21},{"id":"f5e7d77e","cell_type":"code","source":"# train_dataset = OurDataset(split='train', fold=2).load()\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n# train(cfg, train_dataset, dev_dataset)","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:48.141025Z","iopub.execute_input":"2025-04-24T03:46:48.141272Z","iopub.status.idle":"2025-04-24T03:46:48.144559Z","shell.execute_reply.started":"2025-04-24T03:46:48.141253Z","shell.execute_reply":"2025-04-24T03:46:48.143840Z"},"papermill":{"duration":0.029153,"end_time":"2025-01-31T04:06:54.980426","exception":false,"start_time":"2025-01-31T04:06:54.951273","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":22},{"id":"d3540c70","cell_type":"code","source":"cfg = Config()\ncfg.model.name = 'intfloat/multilingual-e5-large-instruct'\ncfg.model.path = '/kaggle/input/model-weights/multilingual-e5-large-instruct_f0_mean.pt'\ncfg.model.pooling = 'mean'\ncfg.train.batch_size = 512\ncfg.train.p_max_seq_length: int = 768\ncfg.train.fc_max_seq_length: int = 768\ndev_dataset = {'dev_cross': OurDataset(split='test', language = 'cross').load()}\npredict_dataset(cfg, dev_dataset)","metadata":{"execution":{"iopub.status.busy":"2025-04-24T03:46:50.206480Z","iopub.execute_input":"2025-04-24T03:46:50.206773Z"},"papermill":{"duration":3560.697076,"end_time":"2025-01-31T05:06:15.690882","exception":false,"start_time":"2025-01-31T04:06:54.993806","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Loading fact-checks.\n272447 loaded.\nLoading posts.\n8276 loaded.\npost_lang\neng    145287\npor     32598\nspa     25440\nara     21153\ntur     12536\n        11567\npol      8796\ndeu      7485\nfra      6316\nmsa       686\ntha       583\nName: count, dtype: int64\ntask lang fact checks 272447\ntask lang post dev 4000\nFiltering by split: 4000 posts remaining and sampled 272447 fact checks\nFiltering fact-checks by language: 272447 posts remaining.\nFiltering posts by language: 4000 posts remaining.\nFiltering posts.\nPosts remaining: 4000\nGet model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1db5128471440a598dc30db5d8ad66e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b82a3a86865846eb924a4897bdb6bf14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d444270211a403f880ecb52a3ce45b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6568d66a784b4e17bc7f84ad88c65e9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94d096def0d84e4cb2fe46a6f64497e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"073152fcb6d046a0ba425ebc64a8277e"}},"metadata":{}},{"name":"stdout","text":"Loading Model Weights\nCalculating embeddings for fact checks\nCalculating 272256 vectors.\n","output_type":"stream"}],"execution_count":null},{"id":"11045a8d","cell_type":"code","source":"cfg = Config()\ncfg.model.name = 'intfloat/multilingual-e5-large-instruct'\ncfg.model.path = '/kaggle/input/model-weights/multilingual-e5-large-instruct_f1_mean.pt'\ncfg.model.pooling = 'mean'\ncfg.train.batch_size = 512\ncfg.train.p_max_seq_length: int = 768\ncfg.train.fc_max_seq_length: int = 768\ndev_dataset = {'dev_cross': OurDataset(split='test', language = 'cross').load()}\npredict_dataset(cfg, dev_dataset)","metadata":{"papermill":{"duration":3522.867809,"end_time":"2025-01-31T06:04:58.579661","exception":false,"start_time":"2025-01-31T05:06:15.711852","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"8bec9f3b","cell_type":"code","source":"","metadata":{"papermill":{"duration":0.014341,"end_time":"2025-01-31T06:04:58.609562","exception":false,"start_time":"2025-01-31T06:04:58.595221","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}